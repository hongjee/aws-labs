{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression in Python\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "### Introduction\n",
    "\n",
    "### Regression\n",
    "- What Is Regression?\n",
    "- When Do You Need Regression?\n",
    "\n",
    "### Linear Regression\n",
    "- Problem Formulation\n",
    "- Regression Performance\n",
    "- Simple Linear Regression\n",
    "- Multiple Linear Regression\n",
    "- Polynomial Regression\n",
    "- Underfitting and Overfitting\n",
    "\n",
    "### Implementing Linear Regression in Python\n",
    "- Python Packages for Linear Regression\n",
    "- Simple Linear Regression With scikit-learn\n",
    "- Multiple Linear Regression With scikit-learn\n",
    "- Polynomial Regression With scikit-learn\n",
    "- Advanced Linear Regression With statsmodels\n",
    "\n",
    "### Beyond Linear Regression\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "### Further Reading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We‚Äôre living in the era of large amounts of data, powerful computers, and artificial intelligence. This is just the beginning. Data science and machine learning are driving image recognition, autonomous vehicles development, decisions in the financial and energy sectors, advances in medicine, the rise of social networks, and more. Linear regression is an important part of this.\n",
    "\n",
    "Linear regression is one of the fundamental statistical and machine learning techniques. Whether you want to do statistics, machine learning, or scientific computing, there are good chances that you‚Äôll need it. It‚Äôs advisable to learn it first and then proceed towards more complex methods.\n",
    "\n",
    "By the end of this article, you‚Äôll have learned:\n",
    "- What linear regression is\n",
    "- What linear regression is used for\n",
    "- How linear regression works\n",
    "- How to implement linear regression in Python, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "Regression analysis is one of the most important fields in statistics and machine learning. There are many regression methods available. Linear regression is one of them.\n",
    "\n",
    "### What Is Regression?\n",
    "Regression searches for relationships among variables.\n",
    "\n",
    "For example, you can observe several employees of some company and try to understand how their salaries depend on the features, such as experience, level of education, role, city they work in, and so on.\n",
    "\n",
    "This is a regression problem where data related to each employee represent one observation. The presumption is that the experience, education, role, and city are the independent features, while the salary depends on them.\n",
    "\n",
    "Similarly, you can try to establish a mathematical dependence of the prices of houses on their areas, numbers of bedrooms, distances to the city center, and so on.\n",
    "\n",
    "Generally, in regression analysis, you usually consider some phenomenon of interest and have a number of observations. Each observation has two or more features. Following the assumption that (at least) one of the features depends on the others, you try to establish a relation among them.\n",
    "\n",
    "In other words, you need to find a function that maps some features or variables to others sufficiently well.\n",
    "\n",
    "The dependent features are called the dependent variables, outputs, or responses.\n",
    "\n",
    "The independent features are called the independent variables, inputs, or predictors.\n",
    "\n",
    "Regression problems usually have one continuous and unbounded dependent variable. The inputs, however, can be continuous, discrete, or even categorical data such as gender, nationality, brand, and so on.\n",
    "\n",
    "It is a common practice to denote the outputs with ùë¶ and inputs with ùë•. If there are two or more independent variables, they can be represented as the vector ùê± = (ùë•‚ÇÅ, ‚Ä¶, ùë•·µ£), where ùëü is the number of inputs.\n",
    "\n",
    "### When Do You Need Regression?\n",
    "Typically, you need regression to answer whether and how some phenomenon influences the other or how several variables are related. For example, you can use it to determine if and to what extent the experience or gender impact salaries.\n",
    "\n",
    "Regression is also useful when you want to forecast a response using a new set of predictors. For example, you could try to predict electricity consumption of a household for the next hour given the outdoor temperature, time of day, and number of residents in that household.\n",
    "\n",
    "Regression is used in many different fields: economy, computer science, social sciences, and so on. Its importance rises every day with the availability of large amounts of data and increased awareness of the practical value of data.\n",
    "\n",
    "## Linear Regression\n",
    "Linear regression is probably one of the most important and widely used regression techniques. It‚Äôs among the simplest regression methods. One of its main advantages is the ease of interpreting results.\n",
    "\n",
    "### Problem Formulation\n",
    "When implementing linear regression of some dependent variable ùë¶ on the set of independent variables ùê± = (ùë•‚ÇÅ, ‚Ä¶, ùë•·µ£), where ùëü is the number of predictors, you assume a linear relationship between ùë¶ and ùê±: ùë¶ = ùõΩ‚ÇÄ + ùõΩ‚ÇÅùë•‚ÇÅ + ‚ãØ + ùõΩ·µ£ùë•·µ£ + ùúÄ. This equation is the regression equation. ùõΩ‚ÇÄ, ùõΩ‚ÇÅ, ‚Ä¶, ùõΩ·µ£ are the regression coefficients, and ùúÄ is the random error.\n",
    "\n",
    "Linear regression calculates the estimators of the regression coefficients or simply the predicted weights, denoted with ùëè‚ÇÄ, ùëè‚ÇÅ, ‚Ä¶, ùëè·µ£. They define the estimated regression function ùëì(ùê±) = ùëè‚ÇÄ + ùëè‚ÇÅùë•‚ÇÅ + ‚ãØ + ùëè·µ£ùë•·µ£. This function should capture the dependencies between the inputs and output sufficiently well.\n",
    "\n",
    "The estimated or predicted response, ùëì(ùê±·µ¢), for each observation ùëñ = 1, ‚Ä¶, ùëõ, should be as close as possible to the corresponding actual response ùë¶·µ¢. The differences ùë¶·µ¢ - ùëì(ùê±·µ¢) for all observations ùëñ = 1, ‚Ä¶, ùëõ, are called the residuals. Regression is about determining the best predicted weights, that is the weights corresponding to the smallest residuals.\n",
    "\n",
    "To get the best weights, you usually minimize the sum of squared residuals (SSR) for all observations ùëñ = 1, ‚Ä¶, ùëõ: SSR = Œ£·µ¢(ùë¶·µ¢ - ùëì(ùê±·µ¢))¬≤. This approach is called the method of ordinary least squares.\n",
    "\n",
    "### Regression Performance\n",
    "The variation of actual responses ùë¶·µ¢, ùëñ = 1, ‚Ä¶, ùëõ, occurs partly due to the dependence on the predictors ùê±·µ¢. However, there is also an additional inherent variance of the output.\n",
    "\n",
    "The coefficient of determination, denoted as ùëÖ¬≤, tells you which amount of variation in ùë¶ can be explained by the dependence on ùê± using the particular regression model. Larger ùëÖ¬≤ indicates a better fit and means that the model can better explain the variation of the output with different inputs.\n",
    "\n",
    "The value ùëÖ¬≤ = 1 corresponds to SSR = 0, that is to the perfect fit since the values of predicted and actual responses fit completely to each other.\n",
    "\n",
    "### Simple Linear Regression\n",
    "Simple or single-variate linear regression is the simplest case of linear regression with a single independent variable, ùê± = ùë•.\n",
    "\n",
    "The following figure illustrates simple linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAE0CAYAAACvs32dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzs3XdYFOfax/EvqCgqiiaIvWONJWLsGpWgNFvU2GNyYomSYAPBFKNooojdIJZoXltQEaMoFjSaRLGbGHvsCNGADZQmbd8/ODuHhQWWupT7c13nOmF25pnnGXf3t1Nvg4iICBVCCCFEMWKo7w4IIYQQeU3CTQghRLEj4SaEEKLYkXATQghR7Ei4CSGEKHYk3IQQQhQ7Em757MSJE5iamrJgwQK9rN/U1BR7e3u9rLugBAcHY2pqyqRJk/TdlXyh7/eQKFpKwmdeFxJu2ZScnMzmzZtxcHCgQYMGvPnmmzRq1IjOnTszadIk/Pz89N1FkUPbtm0rsBBRr6u4BnJOqH+kpP5f1apVadiwIf369WP79u2oVHJbrtBNaX13oChJTk5mxIgRHD58mEqVKmFjY0PNmjWJjIzk/v37+Pv7c+HCBQYPHqwsY2lpyblz53jjjTf02PPirWbNmpw7d45KlSrpuyv5oqS9hypVqqSEfkJCAnfu3OHAgQOcOHGCP//8Ew8PDz33sHA7d+4cxsbG+u6G3km4ZcOuXbs4fPgwb731FgEBAVSuXFnj9bi4OE6fPq0xrXz58jRp0qQgu1nilClTplhv45L2HqpcuTKzZs3SmHbixAkGDBjAunXrmDx5MvXq1dNT7wq/kvReyYwclsyGs2fPAjBy5Mh0wQZQrlw5evXqpTEto/Ml9vb2mJqaEhwczI8//kiXLl0wNzfHwsICJycnIiIitPbhyJEj9OnThxo1alC/fn1GjhzJrVu3mDRpEqamppw4cUKnsagPr/bt25e6detibm5O586dWbp0KfHx8Tq1AdChQwfefPNNwsPDtb6+ceNGTE1NmT9/vjLt999/x8nJiQ4dOlCnTh2qV69Op06d+O6774iNjU3XxoIFCzA1NWXbtm0cPHgQGxsb6tSpo3zBZXTO7c6dO8yZM4eePXvSqFEjqlWrxltvvYWTkxMhISEa806aNAlHR0cAPDw8NA6Npd2me/fuZcCAAdSvX59q1arRrl075syZw8uXL3XebtmR1++hsLAw3NzcaNeuHebm5tSrV49Bgwbx22+/pZs3MjKSFStW4ODgQPPmzTEzM6NRo0YMHz5c+TykZWpqSqtWrYiIiMDFxYWWLVvyxhtvsHr16hxvg+7du2NhYYFKpeLSpUta5/n9998ZPnw4jRo1wszMjLfeeosZM2YQFhamdf4LFy4wcOBAateuTZ06dRgwYADnzp3TeL+l1qpVK0xNTYmLi2P+/Pm8/fbbmJmZ4ebmpsyT3c/ViRMnGDZsGC1btqRatWo0btyYnj178uWXX2ocgo2MjMTDw4POnTtTp04datWqRevWrRk9enS692dG59xevnzJvHnzeOeddzA3N6du3bo4ODiwb9++dPOqP1P29vY8e/aMKVOm0LRpU6pVq0anTp3YvHmz1m1amMieWzZUrVoVgLt37+ZZm9988w3Hjh3DxsaGXr16ceLECTZv3qwciknN19eXCRMmULZsWQYOHEiNGjU4f/481tbWvPXWWzqvMzExkdGjR3Po0CEaN27M4MGDKVu2LEFBQbi7u/Pbb7/h5+dH6dJZvz1GjBjB3Llz2bFjB59//nm61318fICUHwRqK1as4NatW3Ts2JG+ffsSFxfHmTNnWLRoESdOnGDfvn1a171nzx6OHTtGnz59+PjjjzP80lLbt28fGzdupHv37nTo0AEjIyNu3LjBli1bOHjwIL/++iu1atUCUoIiMjKSAwcO0LVrV7p166a0U7duXeW/Z8yYwYYNG6hVqxYODg6Ymppy4cIFli9fTmBgIIcPH8bExCTL7ZaXsvMeunbtGoMGDeLJkyf07t0bOzs7nj9/TkBAAAMHDmTlypWMGTNGmf/WrVvMnz+fLl260LdvX0xNTQkJCeHAgQMcOXIEHx8f+vTpk65P8fHx9O/fn5cvX9KnTx+MjIyoWbNmnoxX23tj+fLlzJkzhypVqtCnTx/Mzc25du0aGzZs4ODBgxw5ckT5t4aUUBkyZAiJiYn069ePhg0bcv36dfr160ePHj0yXf+HH37I5cuXsbKyokqVKtSvXx/I/ucqMDCQYcOGYWJigq2tLbVq1SIiIoK7d++ydu1a5s6dS+nSpVGpVAwZMoTz589jaWnJ6NGjMTIy4vHjx5w6dYrffvuN7t27Z9rniIgIbGxsuHnzJq1bt+bTTz8lMjKSPXv2MGbMGGbOnMkXX3yRbrnIyEj69u2LkZER/fv35/Xr1+zduxcnJycMDQ0ZPXp0Vv9ceiPhlg39+vVj+fLlbNy4kZcvX2Jra0vbtm1p0KABBgYGOWrz4sWLnD59WvngqT9sp06d4sKFC7Rv3x5I+dU1Y8YMSpUqxaFDh2jbtq3Sxrx581iyZInO61y2bBmHDh1i/PjxLFy4kFKlSgEpvzqnTZvGpk2b+OGHH/j000+zbGv48OHMnz8fHx+fdOF2584dzp8/T+fOnWnYsKEyfcmSJdSrVy/dNnN3d2fp0qXs3btX47yl2tGjR/H19eW9997TaZzDhg1j8uTJlC1bVmP6kSNHGDZsGIsXL2bZsmUAODg4KOHWrVu3dIfFAHbs2MGGDRtwcHBg/fr1Guc1PD09+fbbb1mwYAHfffedTv3LK7q+h5KSkhg7diyRkZHs27dPI8D//fdfrKyscHFxoW/fvlSrVg1IOcR18+bNdOf7Hj58yHvvvceXX36pNdzCwsJo1qwZhw4donz58rkeY1BQELdv36Zs2bJYWlqme23u3Lm88847+Pr6Ympqqry2fft2Pv30U1xdXdm6dSuQ8j53cnLi9evX+Pj4YGtrq8y/adMmpkyZkmlfQkNDCQoKSrdNsvu52rx5MyqVin379tGmTRuNtp4/f66E4LVr1zh//jy2trbKj0U1lUrFixcvstx+c+bM4ebNm4waNYrvv/9e+ey5uLjQu3dvPD096du3b7pte/XqVT766COWLFmijGfy5Ml07dqVlStXFupwk8OS2dC6dWvWrVtHtWrV8PX15T//+Q/t2rWjfv36DBs2jD179mT7aq6ZM2dq/KIsXbq08ob5448/lOkHDhzg5cuXDB48WCPYAKZPn67xgc5McnIya9aswczMjAULFihvWABDQ0Pc3d0xMDBgx44dOrVXo0YNevXqxfXr19MdLvrpp5+AlL271OrXr6/1x8Bnn30GwLFjx7Suy9bWVudgg5QLTdIGG4C1tTXNmjXLcD0ZWb16NaVKlWLVqlXpTthPnz6dN954g507d2arzbyg63soMDCQO3fu8Mknn2gEG0D16tX5/PPPiYuLY+/evcr0ypUra72QpW7dugwYMIDbt2+nO8SrNm/evBwFW2RkJAsWLGDBggW4u7szduxYBg4ciEqlYt68eVSvXl1j/jVr1qBSqVi2bFm6z8Hw4cNp3bo1Bw8eVA4bnzlzhvv379O5c2eNYIOUvTILC4tM+/fFF1+k2yY5+VwZGqZ8/WrbRuqjRFnNZ2BgoDGvNgkJCezcuZPy5cszd+5cjc9erVq1mD59OiqVSuuhxvLlyzN//nyN8TRr1oxOnTpx69YtXr16lem69Un23LJp0KBBODg4cOLECU6fPs21a9c4c+YMhw8f5vDhw/Tp04etW7diZGSkU3tpgwpQvqhSnzO5fPkyAJ07d043f4UKFXjrrbc4efJkluu7c+cOz549o0GDBnh6emqdx9jYmNu3byt/r169msjISI157O3tad26NZByyPHo0aP89NNPyniSk5OVD9SgQYM0lo2OjmbNmjXs27ePu3fvEhUVpfGj4PHjx1r7pd4D0ZVKpWLnzp389NNPXL16lYiICJKSkpTXdf03AoiNjeXy5ctUqVKFNWvWaJ1Hfajo+fPnWX7h5CVd30Pqc2ShoaFab3e4d+8ekHIoMrUzZ86wZs0azp8/z5MnT9KdO3r8+DF16tTRmFa2bFlatWqVg9GkHKVIe0WkoaEh3t7eDB8+PN38Z8+epXTp0uzbt0/r+aP4+HiSkpK4d+8ebdu2zfSzZGBgwDvvvKPx/k9L2/swJ5+rDz74AH9/f6ysrBg0aBDdu3fnnXfeSXexTNOmTWnbti1+fn48fPgQOzs7OnbsSLt27ShXrlyG/VS7desWMTExtG/fnjfffDPd6z179gTgr7/+Svdao0aNqFixYrrp6vdXZGRkgR+G15WEWw6UKVOG3r1707t3byDli9zf3x9HR0cCAwPZuHGjTof0AK2Xr6t/JaX+Ilb/QjIzM9PajvowUlaeP38OwP3793W+pNrb2zvdr/O6desq4aa+sGHXrl3Mnz8fIyMjfv/9d0JDQ/nggw803vwJCQn079+fixcv0qJFC95//33efPNN5RCMh4cHr1+/ztUY1b744gu8vb2pXr06VlZW1KhRQ/ky+OmnnzLc49DmxYsXqFQqnj9/nuV2i4qKKtBw0/U9pP639/f3x9/fP8P2oqOjlf/et28fY8eOVS6Wql+/PuXLl8fQ0JCTJ08SFBSk9d/LzMwsx4fq69Spw5UrV4CU9/2pU6f4/PPPmTp1KnXr1qVLly4a8z9//pzExESd/l3Ubar7qE1W7zNzc/N003LyuXJwcMDPz49Vq1bh4+PDpk2bAGjRogWurq4MGDAASPm33LNnD0uWLGHPnj3MnTsXQPnh6O7unultIuo91ozGpR6PtguiMrq9Rtv7q7CRcMsDhoaGDBw4kKtXr7J48WJ+/fVXncNNV+qAePLkidbXM7paMS31m9XGxobt27frtIz6iyYjZcuWZfDgwWzYsIFDhw7Rv39/5ZDkqFGjNOY9cOAAFy9eZMSIEXh7e2u89u+//2b6xZCdL8snT56wdu1aWrRoofUij+zebK/ebi1atODUqVPZWrawUI9h8+bN9O/fX6dlvvvuO4yMjDh+/DhNmzbVeG3q1KkEBQVpXS6nwZaWiYkJffv25aeffsLa2poJEyZw/vx5jcPClSpVIiEhQecfK7n9LGkbW04+VwBWVlZYWVkRGxvLxYsXOXr0KBs2bOCjjz7SOC9qamrKvHnzmDdvHg8ePODUqVNs2bKFbdu2ERISkumPFXXfMhqX+sKs4nafqJxzy0PqD01+PEVBvZeU9j46SPmlffXqVZ3aadKkCZUrV+bixYvZuuQ/K+qrIX18fHj16hX79++ndu3a6a7iUh/60vblmtEXZU48ePCA5ORkevXqlS7Y/vnnHx48eJBumcx+jVasWJEWLVpw+/Ztnj17lmf9LEjvvPMOoP09lJF79+7RtGnTdMGWnJzMmTNn8rR/mWnfvj0jR44kNDQULy8vjdfeeecdXr16leWPMLXMPksqlYrz589nu3+5/VwZGxvTrVs35syZw7x581CpVOmudFVT3wK0b98+ateuze+//57utEHavpUvX57r169rfe+qbwHRdni7KJNwy4Zdu3Zx/PhxkpOT070WFhamnJDt2rVrnq/bzs6OSpUqsXv37nQXbixdujTDe5rSKl26NJ9++ilPnjzB2dmZmJiYdPM8e/ZMOS+hK0tLS5o3b86RI0f44YcfiImJYfjw4crJcDX1ZfVp78158OAB33zzTbbWmRn1es6cOaMRVlFRUUyZMoXExMR0y6gP7YSGhmpt09HRkYSEBCZPnqz1CrVXr15x4cKFvOh+vrCzs6Nhw4b8+OOPGX5x/vXXX8ohNkjZjvfu3ePRo0fKNJVKxcKFC7l582a+9zk1V1dXjIyMWLlypcb2V9+fOHXqVP755590y6V9uEKnTp1o0KABp0+f5uDBgxrzbt68OdPzbRnJyefq119/1Tqfek9KfQj9wYMHXL9+Pd18UVFRREdHU7p06Uxv2ylTpgzDhg0jJiaGuXPnpju/vWzZMgwMDAr1lY85IYcls+HChQusWbMGc3NzOnXqpHETcWBgILGxsXTo0IHx48fn+borVarEkiVLmDBhAra2thr3uV2+fJmuXbsSFBSULky0cXFx4fr162zevJnAwEB69OhBrVq1ePr0Kffv3+fMmTOMGzdO+YWrqxEjRjB79mzlUvjU97ap2djY0LBhQ1avXs2NGzdo3bo1oaGhysU4GQVLdpmbmzN48GD8/Pzo3r07vXr14uXLlxw/fpxy5crRqlWrdL/0O3ToQMWKFdm9ezdGRkbUrl0bAwMDhg0bRt26dRk1ahR//fUX69ato23btlhZWVG3bl0iIyN5+PAhp06dolevXsohWV2cOXMmw+dLNmnShGnTpuVqO6RWpkwZtm7dyvvvv8/IkSNp3749bdq0oUKFCvzzzz9cvnyZ27dv8/vvvyvnDCdPnsy0adN499136d+/P6VLl+bs2bP8/fff2NjYcOjQoTzrX1bq1q3L2LFjWb9+PcuXL1fOPfXo0YN58+bxzTffYGlpibW1NfXr1ycuLo6QkBBOnTpF3bp1lQuuDA0NWblyJUOGDGH06NH0799fuc/t2LFjWFtbc+TIEZ0+S6ll93P11Vdf8fDhQ7p27UrdunUpV64c165d45dffqFq1aqMHTsWSLkcf/To0bRu3ZoWLVpQo0YNIiIiOHz4MC9evMDR0ZEKFSpk2rdvvvmG06dPs3nzZi5fvkzPnj2V+9xevHjBzJkzs33BVmEn4ZYNn3/+ORYWFhw/fpzr169z/PhxYmJiqFKlCh06dGDgwIGMHj2aMmXK5Mv6hw4diqmpKZ6enuzZswcjIyO6dOnCkSNH+PrrrwHdjpuXLl2azZs34+fnx7Zt2zhy5IhyEUSdOnWYNm2a1qvSsjJs2DDc3d1JSEhId2+bWoUKFfD392fu3LmcPHmS06dPU79+fVxcXHB0dGT37t3ZXm9GVq1aRf369dm9ezc//PADb775Jra2tnzxxRcaNyqrVa5cmW3btrFgwQJ2796tXIDQqVMnZU9w0aJF9OnThw0bNnDy5ElevHhB5cqVqVmzJp988glDhw7NVh/v37/P/fv3tb7WtWvXPA03SDlnGBQUhLe3NwcOHMDHxweVSoW5uTnNmjVT3uNqH3/8MUZGRnh7e+Pj40O5cuXo3LkzXl5e+Pv7F2i4ATg7O7Nt2zbWrVvHp59+So0aNYCUz2anTp1Ys2YNp0+f5tChQ1SsWJEaNWowdOhQ3n//fY12unfvTkBAAPPnz+fIkSNAytGHffv24evrC2T/HFR2P1czZswgICCAP//8UzmSUbNmTSZNmsTkyZOpXbs2AG+//TYzZszg5MmTHD9+nBcvXlC1alWaNGnCd999x8CBA7Psm6mpKYcPH2bFihX4+/uzevVqypYtS+vWrZk4caLO52CLEoOIiAh5zHYRl5SURJs2bXj8+DEhISF5ctOsECVV3759OXv2LBcuXKBx48b67o7IITnnVoRERkamO0avUqnw9PQkNDQUa2trCTYhdBAbG6v1PPW2bds4e/YsLVq0kGAr4uSwZBHy559/8uGHH9KrVy/q1q1LdHQ058+f58qVK1StWpVvv/1W310Uokh4/PgxXbp0oWfPnjRs2JDExESuXLnC6dOnMTY2ZunSpfruosglOSxZhDx8+JDvvvuOc+fOER4eTnx8PObm5lhZWTF9+nSNB/wKITIWGRnJnDlzOHXqFI8fPyYmJgYzMzO6devGtGnTaNGihb67KHJJwk0IIUSxU2jOuS1ZsgRTU1NcXFyUaSqVigULFtCsWTOqV6+Ovb09N27c0GMvhRBCFAWFItzOnz/Ppk2baNmypcb0FStW4OXlhYeHB8eOHcPMzIxBgwYV6idRCyGE0D+9h1tkZCTjx49n1apVGuUqVCoV3t7eTJ06lQEDBtCiRQu8vb2Jiopi165deuyxEEKIwk7v4aYOr3fffVdjenBwMGFhYcqT9yHl+WtdunTJsLy9Wk4en1McyLhLjpI4ZiiZ4y6JY4bcj1uvtwJs2rSJe/fusXbt2nSvqZ+vlrYshZmZWYb1vuB/G0TeECVLSRx3SRwzlMxxl8QxQ8q4syoemxG9hdvt27dxd3fn4MGDmRaNTFteQqVSZVpOw8LCIlcbpCiTcZccJXHMUDLHXRLHDLkft97C7dy5czx79kyjGm5SUhKnTp1i48aNSjmN8PBw5RlrAE+fPs2wyGBaCQkJWp+6XVyVK1cu09IXxVVJHHdRGXP58uXz7VmrQmRGb+Fmb2/P22+/rTHN0dGRRo0aMX36dBo3boy5uTnHjx+nXbt2wP9KV7i7u+u0jujoaCpXrpxnhRMLu7Jly+pUdr64KYnjLgpjVqlUREZGalwoJkRB0Vu4mZqapnvTly9fnipVqihPB5g0aRJLlizBwsKCxo0bs3jxYipUqMCQIUN0WoeBgUGJCTYhChv5/Al9KtTPlpwyZQqxsbG4uLgQERGBpaUlu3fvTldZWQghhEitUIVbQECAxt8GBgbMmjWLWbNm6alHQgghiiK93+dWnE2ZMgV7e/tM54mIiMDHxyfbbQcHBzNgwACt0+vVq4eDgwNWVlasXr06220LIURRJ+EG+N70pdWGVlRZXoVWG1rhe9M3123Gx8dz9epVKlasSEhISIbzRUZGsn379lyvL7W2bduyf/9+AgMD2bhxI9HR0XnavhCieMuP78SCVuLDzfemL05HnQh5FYIKFSGvQnA66pTrf8zDhw9ja2vLyJEjNR4XNmfOHKytrXFwcOCXX37By8uLS5cuYW9vz+HDh/n888+VedVXkz579oz+/ftjb29P3759uXPnjk59iImJIT4+nqSkJADmzp2LnZ0d1tbWHDp0CIAbN24o/RkyZAjBwcH06tWL8ePH07NnT7y9vYGUK9+mTp2KjY0Nffr04eLFiwQHB9O7d28cHR3p0aOHspeYtk01bevX5ptvvmHfvn1AyhWvPXr0QKWS4hVCFIT8+k4saCU+3NyD3IlNjNWYFpsYi3uQbrcbZMTPz49hw4ZhY2PDkSNHAAgMDCQ0NJTAwED2799Pz549cXR0pG3btgQEBGRYRbtSpUrs2rWLgIAAnJ2dWbZsWabrvnTpEra2tjRr1oyJEydSqVIljh49SkREBAcOHGDv3r3MmzcPlUrFL7/8wqhRo9i/fz87d+4EUurGLV26lCNHjrBt2zaePHlCQEAACQkJHDp0iHXr1inVGx49eoSnpyeBgYGsWbMGQGubGa1fm7Fjx7J161YA9uzZw6BBgzAwMGDlypXZ/FcQQmRXfn0nFrRCdUGJPoS+Cs3WdF1ERkZy9uxZpk6dCqSExZUrV7hx4wbdu3dXLo8uVaqUxnIZXTYdGRmJs7MzYWFhJCQkULFixUzX37ZtW/bu3culS5eYP38+jo6OXL9+naCgIOUcYHx8PM+fP2f06NEsXryY8ePH07JlSwYNGkSTJk2UK1JbtGhBcHAwd+7coWPHjgDUr1+fiIgIAJo0aaKEsno8aducOnVqhut/44030vW/YcOGJCQk8OjRI7Zv384PP/zApUuXqFq1ahZbXgiRW+rvviuDrwDQyq+VxvSiosTvudU2qZ2t6brYu3cv06ZNw8/PDz8/P7y8vPD19aV58+YEBQUp8yUnJ2NkZERiYiIAVapU4dGjRwBcvnxZmb5jxw5at27NwYMHmTlzps6H6Nq2bYu5uTmBgYE0a9aMXr16ERAQQEBAAEFBQbzxxhsYGRkxf/581q9fz/Hjx4mKiuLWrVtERUWRmJjI9evXqVevHo0bN1YeWP3gwQMqV64MaA/ktG1eu3Ytw/VndD5y9OjRzJs3j8qVK2Nubs7NmzelOrIQBSA/vhP1ocSH2+yuszEubawxzbi0MbO7zs5xm76+vrz33nvK3506deLgwYO89957VK9eHWtra/r168fx48cxNzfH2NiYMWPG8PTpU0xMTLCzs2Pfvn2ULp2yY927d2/8/PwYOnQov//+e7b6MnnyZJYtW0afPn0wMTHB3t4eBwcHPvvsMwB27dqFra0ttra2VKlShbJly1K3bl2mTJnCe++9x4gRIzAzM8POzo5SpUphY2PD+PHjWbRoUYbrTNumhYWF1vUnJCQwbNgwrW04ODhw5MgRxowZA8CtW7do2rRptsYuhMi+/PhO1AeDiIiIYnemXv3AzcjISGUPIzO+N31xD3In9FUotU1qM7vrbIY2G1oAPc1bcXFxuX4kU3BwME5OTuzduzePepWxM2fO8NdffzFx4sR0r71+/RobGxuOHj1KqVKlcHNzY+HChVrbyYtxFzVFZcy6fgZ1VRIfIqyPMfve9KVThU4A2AXa6eU7scg+OLkwGdpsaJEMs6KuU6dOdOrUKd30y5cvM3PmTCZOnKicx8so2IQQeW9os6FERUUBcOWTK3ruTc5IuAkN9erVK5C9tsy0bt0601sFhBD5L6sL1wq7En/OTQghRPEj4SaEECKdqKgo5dBkUSSHJYUQQqTz4sULoOgenpQ9NyGEEMWOhJsQQohiR8JNCCFEsSPhJoQQQmHSpAmVTU2VvyubmlLZ1BSTJk302Kvsk3ATQgihMAwPz9b0wqrEh5v6V0ra/+XmV4q6Gra9vT09e/bE1zdndZDU9dzCwsL48ssvtc6Tk0re6naLExcXF2xtbTlw4ADnz5/nu+++y3KZZ8+eMW7cuFytN6OK6HkhP9sWIjPF4ZmMJT7c8utXirpGm7+/P+7u7soT/gGleKiuzM3N+fbbb7W+lh+VvLPbv8Lg2LFjHDx4EDs7O5YvX8748eOzXOaNN96gYsWKXLmiv8cLTZo0ieDgYL2tX4jUkpOT+Rl4G2jVqhW3W7XSd5dyrMSHW36rVKkS5ubmXLp0iV69ejFhwgScnJyA9JWpk5OTmTBhAnZ2dsyZM0dpI/Uv+MwqeauLourabmrqCtyZ9U9bhe3CULn7iy++4J9//sHe3p5r167x/PlzzMzMOHPmDEOGDCE5OZktW7bg6uqabllra+tcP27s5cuX6cafV3RpW5dxCpGZ5ORk/P396d69O+8DfwFTP2lQAAAgAElEQVQ1gcQslivM5CbufPbo0SOePn3Km2++ycOHD9m7d2+6ytgxMTFYW1uTkJBA+fLlOXDgAGfOnMHPz0+jrdSVvA0MDEhKSqJx48b8/fff7N27l7i4uBy1q5ZV/4YPH86oUaP46KOPSE5O1ljO39+fcuXK0atXL4YMGcLZs2eVyt0PHjzgP//5Dz/++COPHj1i//79GBoa0qFDByZPnqxU7k7drrb19+3bV2v9uO+++47jx48TEBDAH3/8QZ06dYD/PZh5xowZ3LhxQyPEVq5ciZOTE40bN1aqfqvFxsZqhKyara2tUioo7XZLPf533nmHdevWYWhoiIeHR66eiq9t296/f5+FCxcyatQoBg8enOE41WMUIiPJycns378fDw8Prl27BkAtYBbwCVD4605kTMItn1y6dAkHBwcMDAxYsWIFBgYGNG/enEqVKgForUx99+5d2rVrB0D79u3TfZFnVck7p+2qZdW/ESNGKIf81BW2Ab1X7s7M+PHjad68Od9//z1ly5YF0KjqrVKp0m0PY2NjAgICdF5H2vEvXLiQNWvWcP/+ffbv38+oUaM05o+MjGTkyJFASlmP27dvU7ZsWRwcHJg0aVKmbQcHB9OhQwccHR2VbaptnFK5XGQmOTmZffv24eHhwfXr1wGoWbMm06dPZ5KHB+WfPEm/TLVqBd3NXJFwyydt27bV2FMIDg7WCCN1ZWoPDw8g5cv70KFD/Prrr3z44Yf88ccf6SpuN2/enF27djF27FggfSXvnLarllX/EhMTmT9/PgADBgzA2tqaihUrKpW7y5Urp1Tu/vfffzl48CAffvihzpW7U7erbf1GRkaEhIQoe2baWFhY8PDhQ+XvmTNn8u233/L9999jZWVF5cqVNap63717l+bNm2u0kd09t7Tj79ixI2+++SZxcXGcOHEi3fyVK1dWwnPSpEm4ublRr149rePRtm21STtOqVwutNEWarVq1WLatGmMGTOGsmXLkjBuHJF67mdeKPHhllytmtaLR/L7V0qfPn04d+4c9vb2GBgYULNmTby9vfH398fOzo727dsrlbhTL3Py5Emsra0pV64cU6dOpVevXkol7w8//DBH7erav27duilXZpqbm2NhYcHjx4+Vyt13797VqNwdGBiIjY0NSUlJWVbuTttuy5Yt063fy8uLYcOGcerUqQzbMjExoWrVqoSHh/PLL79QqVIlPv74Y+UDvHHjRm7dukW/fv2AlEO9n3zyiUYb2d1zSzv+iIgInj59yr///ou5ubnO7ejStpmZWbp5fHx80o2zfv36yhiFUJ9TW7RokUaoTZ8+ndGjRytHNYoTqcRdjOijOnNhqNyddtznzp3jyJEjGd4+oa7q/ezZM2bOnMmGDRvytJ8PHjxQ9jgXLFiAaaobYvPCrVu3+Oqrr4iPj+frr7/G0tIy3TyZVS4vSFKJO/dyM2Z1qHl4eHDjxg0AateuzfTp0xk1alShDrXc/ltLuBUjxT3cMqKPcetbURmzhFvu5WTMycnJ7N27l0WLFhW5UFPL7b91iT8sKXKnMFTuFkKkSEpKYu/evXh6emqE2owZMxg5cmSRCLW8IuEmhBBFnDrUFi1axM2bN4GUUHN2dmbkyJEYGRnpuYcFT8JNCCGKqKSkJPbs2cOiRYv4+++/AahTpw7Ozs6MGDGiRIaamoSbEEIUMUlJSfz8888sWrSIW7duARJqaUm4CSFEEaEt1OrWrYuzszPDhw+XUEtFwk0IIQq5pKQkdu/ejaenZ7pQGzFiBGXKlNFzDwsfCTchhCikkpKSOHjwIFu2bOH27dtAyhXKM2bMkFDLgoSbEEIUMomJifj5+bF48WKNUFMffpRQy5qUvMkHqYuV2tvb079/f63zpS40mllBUl1FRkYWy8KlebFt1GWD8qKttLJTKDWvCqQOHTo0V21k1nZGBVKfxz7n2pNr/Bn2J9eeXON57PN86UNJlpiYyI4dO+jUqRMTJ07k9u3b1KpVi++//54LFy4wZswYCTYdyZ5bPkn74GRt1IVGR4wYkWlBUl2lbi8/JCUlaa1EkN/L5sW2yY+21I4dO8bFixcBGDVqFMuXL89w3tQFUlvpoRBkVg9qzsjz2OeEvAohWZVSkig+OZ6QVyEAVDWW6gO5lZiYyK5du/D09OTu3btASjUNZ2dnLC0t0z3cW2RN9twKiLaCnKkLja5fv175xawu6jl58mQ6d+7Mzp07+fTTT+nRowdLly4FUvYA+vfvj729PX379uXOnTusXbtWae/w4cNAzgqXppa2iKm2AqJJSUmMGzdOaVO9N6jLstq2S9ppqfcmtBVBffjwodYiqBmNJ+121racroVSXVxcsl0otSgWSH0c9VgJNoArF6/g8okL/7z8Rwqk5kJiYiI+Pj507NiRTz/9lLt379KgQQO8vLw4f/48o0eP1ulB5yK9ErPV8urhtalraGVGHTKQUiepTZs26QpyOjo6KoVGg4OD2b9/v7L848ePOXDgAJGRkbRu3ZrLly/zxhtv0L59e6ZPn06lSpXYtWsXRkZGHDlyhGXLljFlyhTu3LmjfHHmpnBpauoipufOnSMgICBdAdGAgABMTEy0tpnVstoKlaadFhISorQXEBCQrgjqmjVrtBZB1YW25bJTKNXT05OjR49mq1BqXhRIDQkJYf/+/QVWIPXPi3+yZfUWhn0wDPsB9mAJrSxbsWj2Ip4EP5FHsGVTYmIivr6+eHp6cu/ePQAaNGiAi4sLH3zwgQRaHpAtmE/SHpaMiIhIV5AzMxYWFpQrV45y5cpRo0YNpXSKsbExSUlJREZG4uzsTFhYGAkJCVSsWDFdG7kpXJqauohpRgVE7927p+ytpW0zq2W1FSpNO23QoEFKe9kpgqoLbcvlZ6FUyJsCqRYWFjoXSM1OcVTQXiD1bcu3SRiXwPOnz3kU8wiAgaMHMrT7UFZ7rS5RzyzMjcTERHbu3MnixYuVUGvYsCEuLi4MHTpUQi0PlZgtqeseV37RVpCzatWqGoVGU0v95Zf2i1ClUrFjxw5at27N9OnTCQwMxMvLK08Ll6am/tLPqIBogwYN+O2337S2mdWyMTEx6bZLgwYN0k1Ta9y4sc5FUHWhbbn8LJQKeVMg9fbt2zoXSM1OcVTQXiC1VMVS6bbVqnmr+GrOV+nGJ9JTXyiyePFi7t+/D0io5TfZovkk9WFJAAcHB/z9/YH/FeQsXbq0Umg09Re4Lnr37s24ceM4ffo0TZs2BaBatWpKe+PGjcuzwqVq2tpbt24dDg4O7N27Fzs7O9q1a6f1V3xGy2orVLp9+3aNaanby04R1JzS1te8KpQKeVMgtU6dOgVeILVa+WpEl4qmXKlyBOwOwLyqOZ9N/IwmDZpojE/8j7ZQa9SoES4uLgwZMkRCLR9JPbdiRJ81vhISEihTpgxnzpxh2bJl7Nixo8DWXRDjzqhQalpZFUrNqwKpacdcEAVSZ8+eTUxMDF988QWdOnXSabmSWs8tISFBCbUHDx4AKaE2c+ZMBg8enK1QKypjzmtSrFQLCbeCN2bMGJ49e0Z8fDzLli0r0Mvci0rhzrykrzHHx8cD6PwMw5IWbgkJCWzfvp3FixcTHBwMpBxKd3FxyXaoqRX2MecXKVYqCoUtW7bouwtC6E1CQgI+Pj4sWbJEI9TUe2o5vcdT5Jze7nNbv349Xbp0oU6dOtSpUwdra2vl3ixIuWhiwYIFNGvWjOrVq2Nvb69UlhVCiMIgISGBzZs3Y2lpiZOTE8HBwVhYWLB+/XrOnj3LBx98IMGmJ3oLt5o1azJ37lx+++03jh8/To8ePRg1ahRXr14FYMWKFXh5eeHh4cGxY8cwMzNj0KBBvHr1Sl9dFkIIIH2oPXz4kCZNmrB+/XrOnDnD0KFDJdT0TG+HJVNfSQjw9ddfs2HDBs6fP0/Lli3x9vZm6tSpytMkvL29sbCwYNeuXXz88cf66LIQooQxadIEw/Bw5e94YBPwnaEhD/770IEmTZowc+ZMBg0aJIFWiBSKx28lJSXh5+dHdHQ0HTp0IDg4mLCwMHr37q3MY2xsTJcuXTh79qweeyqEKEnUwRYPrAeaABOAB8nJNG3alA0bNnD69GmGDBkiwVbI6PWCkmvXrtGnTx/i4uKoUKECW7dupWXLlkqAqe+vUTMzM+Px48f66KoQAkrcfVnxwP8B3wHB/53WHPgGsD51SgKtENPrO9XCwoITJ04QGRmJv78/kyZN0ni+orYnc2T1JAp17aPbt29Trly5EvdYoLi4OH13QS9K4riLwphfvnxJeKrDenlB/RnPTwkJCezbt49twEPgypUrAFxr1YohQCngwn8fn1UQCmLMhVFubgfQa7gZGRnRsGFDIKWu2B9//MHq1atxdnYGIDw8nNq1ayvzP336NN3eXFoWFhYa97mVpPufSuL9XlAyx11UxlypUqVMH1mWXfl9z1d8fDzbtm1jyZIlhIaGAtAi1evDUv13Qd17Jve55UyhOOemlpycTHx8PPXq1cPc3Jzjx48rr8XFxXH69GnloblCiIKXmJiY4fNQi7LXr1+zceNG2rVrx7Rp0wgNDaV58+bsBK7ou3MiR/QWbnPmzOHUqVMEBwdz7do15s6dy8mTJxk6dCgGBgZMmjSJ5cuX4+/vz/Xr15k8eTIVKlTQ+nDZwkZdidvBwQErK6tM64tpk1G16JxUzc6ssnJ+y06FarW8qFQt8k9ycrJSmqg4eP36NRs2bKBdu3ZMnz5dCbX/+7//IygoiMHVqmn9kkyuVq3A+yqyR2+HJcPCwpgwYQLh4eFUqlSJli1bsmvXLqysrACYMmUKsbGxuLi4EBERgaWlJbt371ZKcRR26pI3SUlJdOzYkbFjx1KhQgWdls2PatH6kJ0K1Wr6rlQtSobXr1+zdetWli5dyj///AOklPdxdXWlX79+GBqmRNqrW7dSFvhvTcFIPVcXEbrTW7hlVT3YwMCAWbNmMWvWrDxZX+qCl2lVqVJFqYcWFRXFixcvMpw3u+cPYmJiiI+PJykpCUip8Hz27FkSEhKYMWMG9erVw8nJibJly1KuXDl27dpFcHAwTk5O/Pzzz3z66aeEhobSoUMHpc0TJ06wc+dOVq1aBaTs0f355588e/aMSZMmkZSURGJiIl5eXpQpU0ZZ7saNG+nWpc0333xD+/bt6devH9HR0dja2vLbb79lq6xM6grVixYt0qhQvXjxYnbu3Mm2bdu4evWq8sBfNXWlagk3kddev37Nli1bWLZsWaahJoo++ZfMJ5cuXcLW1pZmzZoxceJEKlWqpFHhee/evcybN4+jR48yatQo9u/fz86dOzXaCAgIUKpm29jYZHmuQ12dOyAgAGdnZ5YtW6bxurrCtbZ1pTZ27FilUvSePXsYNGiQEmwrV67Uafyenp7UqFGDgIAAXr9+rbVC9bZt23B3d1eWUbfduHFjrl27ptN6hNBFXFwc69ev5+2338bZ2Zl//vmHFi1asGnTJk6ePMmAAQMk2IqZEnPTiq57XBUrVtRa1Tq71IclL126xPz583F0dNRa4XnEiBEsX75ca4XujKpmZ7QHFRkZyVdffZVhdW5tVa+1adiwIQkJCTx69Ijt27fzww8/ACmBXbVqVY15161bx969e2nYsKGyJ5kVbRWqU7etyy0fQugiLi5O2VN79CilgnjLli1xdXXFwcFB50CrUqVKfnZT5IMSE2760rZtW8zNzQkMDNRa4TkxMTFd1Wl1KDVs2FBr1ewqVaooH9TLly8re3R+fn7pqnOnpq0aeMuWLbVWmB49ejTz5s2jcuXKSvHLmzdv0qJFC435JkyYwIQJEzLdBrpUqE7dtrZK1UJkR1xcHJs3b2bZsmXKgx9atmyJm5sb9vb22d5Ly4sfvKJgSbgVgMmTJ+Ps7MzBgwfTVXju1q1bukrU6g+jvb291qrZLVu2xMTEBDs7O7p27apMf/fdd3F0dNSozp2atqrXCQkJWitMOzg4MHPmTI2AvHXrFv369cv2+HWpUJ26bW2VqkXhUNj3qOPi4ti0aRPLly9XPkdvvfUWrq6uOQo1UXRJsdJiJCc39mZUYfr169fY2Nhw9OhR5RFDbm5uLFy4MEd9y6pCtbrtnFSqLio3NOelojLmgipWqi3UWrVqhaurK3Z2drkOtaioKEA/e3ByE3fOSLgVI3n1hXf58mVmzpzJRx99xPDhw/OgZ/mrqHzR56WiMub8DrfY2Fgl1P79918gJdTc3Nyws7PLsz1N9dXWefm0FV1JuOWMHJYU6bRu3ZpDhw7puxtCZCg2Npb/+7//Y/ny5YSFhQH5E2qi6JJwE0LoLD4+Hki5OEkf4uLi8Pb21gi11q1b4+bmhq2trYSaUEi4CSEKvdjYWH788UeWLFnCs2fPAGjTpg1ubm7Y2NhIqIl0JNyEEIVWTEwMP/74IytWrFBK50ioCV0U63AzNDQkPj5eb4dQhCjJ4uPjc3yVorZQa9u2LR9++CEff/yxhJrIUrEOt4oVKxIVFUVsbKy+u1IgXr58SaVKlfTdjQJXEsetrzFHR0cD6PQQcENDw2xfOh8TE8PGjRtZuXKlEmpvv/02bm5u9OnThzt37kiwCZ0U63AzMDAoMlUE8kJ4eLheLlXWt5I4bn2N+eXLlwB5fotNdHS0EmpPnjwBNENN34FW0t5fxUGxDjchROGmLdTatWuHm5sb1tbWeg81UXRJuAkhdJZXDxBWh9qKFSt4+vQpAJaWlri5ufHee+9JqIlck3ATQugst4+fio6OZsOGDaxcubJIhZr66SfVq1fXc0+EriTchBD5TluotW/fHjc3N6ysrAptqKklJCTouwsimyTchBA6y+4DhKOiopRQU9983b59e2bNmkXv3r0LfaiJokvCTQihsxcvXgBZh1tUVBQ//PADq1atUkLtnXfewc3NTUJNFAgJNyFEnlGH2sqVK3n+/DkAHTp0wM3NjV69ekmoiQIj4SaEyLVXr14pe2qpQ23WrFn07NlTQk0UOAk3IUSOvXr1ivXr17Nq1SrlkGXHjh1xc3OTUBN6JeEmhMiSSZMmGIaHE3LlCgCGpqZ8Dyw2MOC5KqXecadOnXBzc+Pdd98tdqGmy+PGROEi4SaEyJLhf5/zqFYfeA6gUtGpUydmzZpFjx49il2oqVWtWlXfXRDZJOEmhMjSS2AVMOC/fz8HugFzAMuDB4ttqImiS8JNCJGhly9fsm7dOryAF8BXrVrRDfgF6AUYAJElINj0XYFcZJ+EmxAincjIyJRQ8/IiIiICgO7AXKAnKaFWkoSFhQFSHaAokXATQigiIyNZu3YtXl5eREZGAtClSxfmnzpVIkNNFF0SbkIIIiMjWbNmDatXr1ZCrWvXrri5udG9e3dMmjTBIM1FJQDJ1aoVdFeF0ImEmxAlWEREBGvXrtUItW7duuHq6kr37t2V+V7duqWvLgqRIxJuQpRAERERyp6aurq2tlAToqiScBOiBImIiMDb2xtvb28l1Lp3746rqyvdunXTc++EyDsSbkKUANpCrUePHri6utK1a1c9906IvCfhJkQxFhERwerVq1mzZo0Sau+++y6urq506dJFz70rOszNzfXdBZFNEm5CFEMRERF4eXmxdu1aCbU8IDdvFz0SbkIUIy9evGD16tUaodazZ09cXV3p3LmznnsnRMGRcBOiGHjx4oWyp/bq1SsAevXqhaurK506ddJz74o235u+vHjxguiEaDbe3sjsrrMZ2myovrslsiDhJkQRJqGWv3xv+uJ01IlzA84BMPePuTgddQKQgCvkJNyEKIIiIiKYN28e69atU0Ktd+/euLq60rFjRz33rvhwD3InNjFWY1psYizuQe4SboWchJsQRcjz58/x8vLC29ubmJgYAKysrHB1daVDhw567l3xE/oqFIBWfq20TheFl4SbEEXAs2fP8PLyYt26dURFRQESagWhtkltQl6FaJ0uCjcJNyEKsWfPnvH999+zfv16JdTee+89Ro4cyfvvv6/n3hV/s7vOxumok8ahSePSxszuOluPvRK6kHATohBSh9q6deuIjo4GwNraGldXV9q3b8/t27f13MOSQX1ezT3IndBXodQ2qS1XSxYREm5CFCJPnz5V9tTUodanTx9cXV2xtLTUc+9KpqHNhkqYFUESbkIUAk+fPmXVqlX88MMPEmpC5AEJNyH0SB1q69evV65+7Nu3LzNnzpRQEyIXDPW14qVLl9KrVy/q1KlDo0aNGDZsGNevX9eYR6VSsWDBApo1a0b16tWxt7fnxo0beuqxEHnnyZMnzJ49m9atW7NixQpiYmLo27cvx44dY8eOHRJsQuRStsItMDCQ5OTkPFnxyZMn+eSTTzh8+DD+/v6ULl2agQMH8uLFC2WeFStW4OXlhYeHB8eOHcPMzIxBgwYpN60KUdQ8efKEr7/+mjZt2rBy5Uol1I4fP86OHTto166dvrsoRLGQrcOSw4YNw8zMjMGDBzNs2DDatm2b4xXv3r1b4++1a9dSt25dzpw5g62tLSqVCm9vb6ZOncqAAQMA8Pb2xsLCgl27dvHxxx/neN0i7/je9JUryXQQHh7OypUr2bhxo3L40cbGBjc3t1x9joQQ2mVrz2379u10796dzZs307t3bzp27MiyZcsIDc393fpRUVEkJydjamoKQHBwMGFhYfTu3VuZx9jYmC5dunD27Nlcr0/knvq5eyGvQlChIuRVCE5HnfC96avvrhUa4eHhfPXVV7Rp04bvv/+emJgYbG1t+fXXX9m+fbsEmxD5JFvh1rdvXzZs2MDff//NqlWrqFGjBvPnz6dNmzb069ePbdu25fiQoZubG61atVKethAWFgaAmZmZxnxmZmaEh4fnaB0ib2X23L2SLiwsjC+//FIJtdjYWCXUfHx8JNSEyGcGERERqtw08O+//+Lr68uOHTu4fv065cqVw87OjhEjRmBlZaVTG1988QW7d+/m0KFD1K9fH4CzZ8/St29frl69Su3a/3vUzeTJkwkLC8PPz09rW3Jza8HpENABFSquDL4C/O/5ewYYcM7+nD67pjdPnz5ly5Yt+Pn58fr1ayClSOi4ceNo1qyZnnsnRNFjYWGRo+VyfStAQkIC8fHxxMfHo1KpMDEx4fTp0/j5+dG8eXPWrVvHW2+9leHys2bNYvfu3ezbt08JNvhfWffw8HCNcHv69Gm6vbnULCwsuH37do43SFFW0OPO7Ll7BdmPwvDvHRYWxooVK9i4cSNxcXEA2NvbM3PmTNq0aZPn6ysMY9aHkjjukjhmyP24c3QrQGRkJJs2bcLOzo62bdvi6elJixYt2L59O9evX+fq1av4+PgQHR3N559/nmE7rq6u7Nq1C39/f5o0aaLxWr169TA3N+f48ePKtLi4OE6fPi0lPQqJ2V1nY1zaWGNacX/unkmTJlQ2NVX+F2NqyhxTU9o0bcrq1auJi4vDwcGB33//nW3btuVLsAkhspatPbeAgAB27NhBYGAgr1+/pn379nh6evL+++8rF4Ko2djYEB4ezowZM7S25ezszI4dO9i6dSumpqbKObYKFSpQsWJFDAwMmDRpEkuWLMHCwoLGjRuzePFiKlSowJAhQ3I4XJGX1FdFOp12IiwmjDomdYr91ZKG/z3f+xhYBKwB4v77moODAzNnzqR169Z66p0QQi1b4TZ69Ghq1aqFo6MjI0aMoHHjxpnO37JlS4YO1f5F98MPPwAol/mrubq6MmvWLACmTJlCbGwsLi4uREREYGlpye7duzExMclOt0U+KmnP3XsMeABr+V+ovQ/MBupv3aqvbgkh0shWuP3888+8++67GBgY6DS/paVlhk9aiIiIyHJ5AwMDZs2apYSdEPry+PFjli9fzv8Br4ErV1Iuoklq1Qr1gcdIPfVNCJFetsKtZ8+e+dQNUZQ9f/4cgKpVq+q5J3nv0aNHLF++nE2bNilXPw5O9bqcUROicNLbsyVF8REdHa08yb64ePToETNnzuTtt99m3bp1vH79mgEDBvAXsEvfnRNCZEmqAgiRyqNHj1i2bBmbN29W9tQGDhyIi4sLLVu2xKRJE9DyEIHkatUKuqtCiExIuAkB/PPPP8rhx/j4eEAz1NRe3bqV8h8hKff3Repw7lgIUfAk3ESJljbUDAwMGDRoEC4uLrRo0ULf3RNC5JCEmyiRQkNDWb58OZs3b1ZC7f3338fFxYXmzZvru3tCiFyScBMlSmhoKMuWLWPLli25CrUqVarkYy+FELkl4SZyrUyZMvruQpbUobZ582YSEhIwMDBg8ODBuLi45OiBxhUrVsyHXgoh8oqEm8i16tWr67sLGQoJCVH21NShNmTIEJydneUp/UIUYxJuolh6+PAhy5YtY+vWrRqh5uLiQtOmTXPdflRUFCB7cEIUVhJuoljRFmpDhw7F2dk5T0JN7cWLF4CEmxCFlYSbyLWQ/97zVadOHb314eHDhyxdupRt27aRkJCAoaFhvoSaEKJokHATRVpwcDDffvst+/fvJzExEUNDQz744AOcnZ3T1QgUQpQcEm6iSAoODlb21FKHmouLS4msWiyE0CThJoqUBw8esHTpUn766Scl1GxtbXF3d5dQE0IoJNxEkfDgwQOWLFmCj4+PEmrDhg3DxcUFlUolwSaE0CDhJgo1baE2fPhwnJ2dlUrwt2/f1nMvhRCFjYSbKJQePHjA4sWL8fHxISkpSQk1FxcXGjVqpO/u6fXKUCFE1iTcRK7l5XMW79+/z+LFi9m+fTtJSUmUKlWKESNG4OzsXChCTQhRNEi4iVzLixuZtYXayJEjcXZ2pmHDhnnQSyFESSLhJvTq3r17LF68mB07dhSpUPv333+Bwv1cTSFKMgk3kWs5ec7ivXv38PT0ZOfOnUqojRo1CmdnZxo0aJBfXc0zCQkJ+u6CECITEm4i17LznEVtoTZ69GhmzI7DUBoAABT2SURBVJhRJEJNCFE0SLiJAnH37l0l1JKTk5VQc3Z2pn79+vrunhCimJFwE/nqzp07eHp64uvrS3JyMqVLl2bUqFHMmDFDQk0IkW8M9d0BUTzduXOHiRMn0qFDB3bs2IGhoSEffvghFy5cYNWqVUU62Hxv+ir/3WpDK42/hRCFg+y5iRwzadIEw/BwQq5cAaCyqSl/A/PKlsUnIUHZUxs9ejTTp08v0oGm5nvTF6ejTpwbcA6AkFchOB11AmBos6H67JoQIhUJN5FjhuHhGn+PBnyA5NevKV26NGPGjGH69OnUq1dPL/3LD+5B7sQmxrLr/i5lWmxiLO5B7hJuQhQiEm4iV26m+u9tpLyhxgGOFy8Wq1BTC30VCsDcP+ZqnS6EKBwk3ESO3Lp1i+Wk7KmpWrWiNDABmAXUByKLYbAB1DapTcirEK3ThRCFh1xQIrLl77//Zty4cXTs2JGfSPl1NBG4A6wlJdiKs9ldZ2Nc2lhjmnFpY2Z3na2nHgkhtJE9N6GTmzdv4unpye7du1GpVJQpU4ZPEhKYBdTVd+cKkPq8mnuQO6GvQqltUpvZXWfL+TYhChkJN5EpbaE2ZswYpk2bRgsrq3QXlQAkV6umh54WnKHNhkqYCVHISbgJrW7cuIGnpyc///yzEmoffvghU6dOVWqZvbp1S8+9FEII7STchIYbN26waNEi9uzZg0qlwsjISAm12rXlogkhRNEg4SYAuH79Op6enhJqQohiQcKthLt+/bqypwZgZGTE2LFjmTp1KrVq1dJz74QQImck3Eqoa9eusWjRIvbu3QtIqAkhihcJtxImbaiVLVtWOfwooSaEKC4k3EqIq1evsmjRIvz9/YGUUFPvqdWsWVPPvRNCiLwl4VbMXblyhUWLFrFv3z4gJdQ++ugjpk6dSo0aNfTcOyGEyB8SbsXU5cuXWbRoEfv37wdSQu3jjz9mypQpEmpCiGJPwq2YSRtq5cqV46OPPpJQE0KUKBJuxcTly5eZPXs2v/76K5ASauo9terVq+u3c0IIUcAk3Iq4v/76Cw8PDw4cOACkhNp//vMfnJycJNSEECWWhFsRlTbUjI2NGTRoEN988w3m5uZ67p0QQuiXXuu5BQUFMXz4cJo3b46pqSnbtm3TeF2lUrFgwQKaNWtG9erVsbe358aNG3rqbeFw6dIlRowYwbvvvsuBAwcwNjbG0dGRS5cuMW3aNAk2IYRAz+EWHR1NixYtWLhwIcbGxuleX7FiBV5eXnh4eHDs2DHMzMwYNGgQr1690kNv9evSpUsMHz6cnj17cvDgQYyNjfnss8/466+/+PbbbyXUhBAiFb0eluzTpw99+vQBYPLkyRqvqVQqvL29mTp1KgMGDADA29sbCwsLdu3axccff1zg/dWHS5cusXDhQg4dOgSkHH785JNPcHJyoloxr5smhBA5VWjPuQUHBxMWFkbv3r2VacbGxnTp0oWzZ88W+3D7888/WbhwIYcPHwZSxj5u3Dg+//xzCTUhhMhCoQ23sLAwAMzMzDSmm5mZ8fjxY310qUD88ccfeHh4KKFWvnx5JdTSbgshhBDaFdpwUzMwMND4W6VSpZuW2u3btzX+v6i4du0a69evJygoCEi5pP+DDz5g1KhRVK1alYiICCIiIrJsp6iNO6+UxHGXxDFDyRx3SRwzpIzbwsIiR8sW2nBTXyARHh6uUSzz6dOnme7BWFhY5GqDFLSLFy/i4eFBYGAgkLKnNn78eD7//HPefPPNbLVVlMadl0riuEvimKFkjrskjhlyP269Xi2ZmXr16mFubs7x48eVaXFxcZw+fZqOHTvqsWd548KFCwwdOhQrKysCAwOpUKECU6dO5fLly8ydOzfbwSaEEOJ/9LrnFhUVxb179wBITk4mNDSUy5cvU6VKFerUqcOkSZNYsmQJFhYWNG7cmMWLF1OhQgWGDBmiz27nyvnz5/Hw8ODo0aMAVKhQgQkTJvDZZ5/xxhtv6Ll3QghRPOg13P7880/69eun/L1gwQIWLFjAiBEj8Pb2ZsqUKcTGxuLi4kJERASWlpbs3r0bExMTPfY6Z9KGWsWKFRk/fryEmhBC5AO9hlv37t0zvUjCwMCAWbNmMWvWrALsVd46d+4cHh4e/PLLL0BKqE2YMAFHR0cJNSGEyCeF9oKSosSkSRMMw8M1pp0GvjEy4kh8PJASahMnTsTR0ZGqVavqoZdCCFFySLjlgdTBdgqYCwQCxMdLqAkhhB5IuOWRIFJC7ch//zYBnIBPLl+WUBNCiAIm4ZZLZ86cYTFwFLhy5QoAO1q1Yhr8f3t3H1N13f9x/EkaqcmNEtdBBbQURZimYFjq5PK2EBPTsTLXylJUdN4sFdS8oxZ3lcHyrsjlimreLZmuTKchcNCcZZalF4yZqIHTxAmDGZzz+4OfZ56061JRz+HD67GxPJ/vAd6vc9IXn/OFL3QELqvYRETuO7f9OTd3V1xczPjx43nmmWfYS+NO7Zo3aSw2ERFxDZXbbbJarcTFxRETE8N3332Ht7c3y4BTrh5MREQcVG63yGq1Mm7cOMaMGUN+fj7e3t4sWrSIY8eOsfJf/7rpTs2mq/eLiLiEzrn9D0VFRaSnp3PgwAEAvL29mTlzJjNnzsTX1xeAK//5T+Ody8sBuHwLFzgWEZF7R+X2D4qKikhLS6OgoAC4eamJiIh7Urn9TWFhIWlpaRQWFgKNpZaYmMiMGTNUaiIizYTK7f8VFBSQnp7uVGqzZs1i+vTpt1xqHTp0uJcjiojILXLLcouNjSUsLIzMzMx7/rkKCgpIS0tz/JJQHx8fEhMTb6vUrmnfvv29GFFERG6TW5bbvWa32x07tetL7dpOzcfHx8UTiohIU7SocrtWamlpaVitVqCx1GbPnk1CQkKTS626uhrQDk5ExNXc9ufc6uvrSUpKomvXrnTt2pVly5Zhs9nu6GPZ7Xby8/MZM2YM48aNw2q14uvry9KlSzl27BgLFy68K7u1S5cucenSpSZ/HBERaRq33blt2bKFSZMmsWfPHo4fP87cuXOxWCzMnj37lj+G3W7nwIEDpKWlUVxcDICvr69jp+bt7X2vxhcRERdy23KzWCxkZGTg4eFBz549KS0tZe3atY5y+/bbb1myZAk2m43ExESmTp3qeN9rO7XrS61Dhw7Mnj2badOmqdRERAznUVVVZXf1EH8XGxtLYGAgGzZscKzl5+cTFxfH6dOnadeuHVFRUeTl5dGxY0eGDRvGjh07CAgIwG63k5WV5ZK5J06cCMC2bdtc8vlFREwRGxtLSEjIHb+/255z+2+OHDlCr169CAwMpF27dowdO5bdu3cD4OHh4eLpRETE1dz2ZckjR45gt9sdZXX48GE6deqEt7c3FRUVBAYGOu7buXNnzp0757g9dOhQQkJC8PLyuuHj3itbTmxx/Pljr49ZPng58aHx9+3zA5SUlDTpK53mqiXmbomZoWXmbomZoTF3U7jtzq2iooLk5GRKSkrYsWMH2dnZJCYmAo3n1P7u+h2bl5fXfS+2OXvnOG6XXylnzt45ToUnIiL3j9vu3OLj47HZbIwYMQIPDw9eeuklR7l16tSJM2fOOO577tw5goODXTUqKUUp1NbX0mdbH8dabX0tKUUp9333JiIiblpuu3btcvz5ZpfgioyM5MSJE5w5cwY/Pz927tzJV199dT9HdHLmypnbWhcRkXvLLcvtf2ndujVvv/02cXFx2Gw2ZsyYQadOnVw2T6BXIOVXym+6LiIi91+zLDeAmJgYYmJiXD0GAMsHL2fO3jnU1tc61tq2bsvywctdOJWISMvltt9Q0pzEh8aTPTKbIK8gPPAgyCuI7JHZOt8mIuIizXbn5m7iQ+NVZiIibkI7NxERMY7KTUREjKNyExER46jcRETEOCo3ERExjspNRESMo3ITERHjqNxERMQ4KjcRETGOyk1ERIyjchMREeOo3ERExDgqNxERMY7KTUREjKNyExER46jcRETEOCo3ERExjspNRESMo3ITERHjNItyy8nJoW/fvlgsFqKjo7Fara4eSURE3Jjbl9v27dtJTk7m9ddf58CBA0RFRREfH095ebmrRxMRETfl9uW2Zs0aXnzxRV5++WV69epFZmYmFouFjRs3uno0ERFxU25dblevXuXo0aMMHz7caX348OEcOnTIRVOJiIi7a+3qAf6bixcv0tDQgL+/v9O6v78/58+fv+n7lJSUOP23pVHulqMlZoaWmbslZobG3CEhIXf0vm5dbtd4eHg43bbb7TesXRMSEtKkB6Q5U+6WoyVmhpaZuyVmhqbnduuXJf38/GjVqtUNu7QLFy7csJu7Xkv8HwGUuyVpiZmhZeZuiZmh6bndutw8PT3p168f+/fvd1rfv38/AwcOdNFUIiLi7tz+ZclZs2Yxffp0IiMjGThwIBs3bqSiooIpU6a4ejQREXFTbl9uEyZM4M8//yQzM5PKykp69+7N5s2bCQ4OdvVoIiLipjyqqqrsrh5CRETkbnLrc24iIiJ3wrhyM/06lEVFRbzwwgv07t0bX19fcnNznY7b7XZSU1MJDQ0lICCA2NhYfvvtNxdNe3e89957DBs2jKCgILp3787zzz/Pr7/+6nQf03J/9NFHDBo0iKCgIIKCghg1ahS7d+92HDct7z9599138fX1ZeHChY41E7Onpqbi6+vr9NazZ0/HcRMzA1RUVDBjxgy6d++OxWJh4MCBFBYWOo43JbdR5dYSrkNZU1NDWFgYaWlptG3b9objWVlZrFmzhvT0dPbt24e/vz/PPfccV65cccG0d0dhYSGvvfYau3fvJi8vj9atWzN+/HguXbrkuI9puTt37syqVavIz89n//79DB06lMmTJ/PLL78A5uW9mcOHD7Np0ybCw8Od1k3NHhISwsmTJx1v139hbmLmqqoqnn76aex2O5s3b+bQoUNkZGQ4/ZhXU3Ibdc5txIgRhIeHk52d7ViLiIggLi6OFStWuHCye6NLly5kZGQwefJkoPGrnNDQUKZNm8aCBQsAqK2tJSQkhDfffNOY7zCtrq4mODiY3NxcYmJiWkzubt26sWLFCl555RXj816+fJno6GiysrLIyMggLCyMzMxMY5/r1NRU8vLyKC4uvuGYqZlTUlIoKipyekXiek3NbczOTdehhN9//53Kykqnx6Bt27YMGjTIqMeguroam82Gr68vYH7uhoYGtm3bRk1NDVFRUcbnBZg3bx5xcXFER0c7rZuc/dSpU/Tu3Zu+ffvy6quvcurUKcDczLt27SIyMpIpU6bQo0cPhgwZwocffojd3rjfamput/9RgFt1J9ehNE1lZSXATR+DP/74wxUj3RPJycn06dOHqKgowNzcx48fZ/To0dTV1fHwww/z2WefER4e7viLbVreazZt2kRZWRkbNmy44Zipz/WAAQNYu3YtISEhXLhwgczMTEaPHs3BgweNzXzq1Ck+/vhjEhMTmTdvHj///DNJSUkAJCQkNDm3MeV2ze1ch9JUJj8GS5Ys4eDBg3zzzTe0atXK6ZhpuUNCQigoKODy5cvk5eUxc+ZMdu7c6ThuWl5ovJ5gSkoKX3/9NZ6env94P9Oyjxo1yun2gAED6NevH59//jlPPPEEYF5mm81G//79HaeMHn/8ccrKysjJySEhIcFxvzvNbczLknd6HUqTWCwWAGMfg8WLF7Nt2zby8vLo1q2bY93U3J6enjz22GOOfwD69OnD2rVrjc0L8P3333Px4kWeeuop/Pz88PPzo6ioiJycHPz8/OjYsSNgZvbrtW/fntDQUMrKyox9vi0WC7169XJa69mzJ2fOnHEchzvPbUy56TqU0LVrVywWi9NjUFdXR3FxcbN/DJKSkti6dSt5eXlO3yINZue+ns1m4+rVq0bnjY2NxWq1UlBQ4Hjr378/EydOpKCggB49ehib/Xp1dXWUlJRgsViMfb6ffPJJSktLndZKS0sJCgoCmv73ulVycvLKuzqxC3l5eZGamkpAQABt2rQhMzMTq9XKBx98gI+Pj6vHuyuqq6s5ceIElZWVfPrpp4SFheHt7c3Vq1fx8fGhoaGB1atX06NHDxoaGli6dCmVlZW8//77PPTQQ64e/44sWLCAL7/8kk8++YTAwEBqamqoqakBGr+o8fDwMC73ypUr8fT0xGazcfbsWdatW8fmzZtZuXIl3bt3Ny7vNW3atMHf39/pbcuWLQQHBzN58mQjn2uAN954w/F8l5aWsnDhQsrKyli9ejW+vr5GZg4MDCQ9PZ0HHniAgIAA8vPzeeutt5g/fz6RkZFNfq6NOufWEq5D+eOPP/Lss886bqemppKamsqkSZNYt24dc+fOpba2loULF1JVVUVkZCTbt2/Hy8vLhVM3TU5ODgBxcXFO60lJSSxevBjAuNyVlZUkJCRw/vx5vL29CQ8PZ+vWrYwYMQIwL+/tMDH7uXPnmDp1KhcvXuSRRx5hwIAB7Nmzx/Fvl4mZIyIiyM3NJSUlhczMTAIDA1myZAlTp0513KcpuY36OTcREREw6JybiIjINSo3ERExjspNRESMo3ITERHjqNxERMQ4KjcRETGOyk1ERIyjchMREeOo3ERExDgqNxERMY7KTaQZqK2tJSoqioiICMdFowFqamro378/UVFR1NXVuXBCEfeichNpBtq2bcv69es5ffo0y5cvd6wvW7aM8vJy1q9fT5s2bVw4oYh7Meq3AoiYLCIigvnz55OZmUlsbCwAGzduZNGiRURERLh4OhH3ot8KINKM/PXXX4wcOZILFy5gt9vx9/dn7969PPjgg64eTcStqNxEmpnjx48zePBgWrduTWFhIaGhoa4eScTt6JybSDOzb98+AOrr6zl58qSLpxFxT9q5iTQjJ06cIDo6mrFjx3L27FlKS0spLi7G39/f1aOJuBWVm0gzUV9fz8iRI6msrMRqtVJVVcWQIUP497//TW5urqvHE3ErellSpJl45513OHr0KFlZWXTo0IFHH32UVatWsWvXLr744gtXjyfiVrRzE2kGfvrpJ0aOHMmkSZPIzs52rNvtdiZMmMAPP/yA1WqlS5cuLpxSxH2o3ERExDh6WVJERIyjchMREeOo3ERExDgqNxERMY7KTUREjKNyExER46jcRETEOCo3ERExjspNRESMo3ITERHj/B/EtaHYI6jdcAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot Single-variate linear regression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np    \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))\n",
    "y = np.array([5, 20, 14, 32, 22, 38])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "y_pred = model.predict(x)\n",
    "\n",
    "# set the style and title\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.title('Single-variate Linear Regression')\n",
    "\n",
    "# prepare the plot\n",
    "plt.plot(x, y, 'go', label='Actual Response, y$_{i}$')\n",
    "plt.plot(x, y_pred, 'rs', label='Predicted response, f(x$_{i}$) = b$_{0}$ + b$_{1}$x$_{i}$')\n",
    "plt.plot(np.insert(x, 0, 0, axis=0), np.insert(y_pred, 0, model.intercept_, axis=0), color='black', linewidth=2, label='Estimated  regression line, f(x) = b$_{0}$ + b$_{1}$x')\n",
    "plt.plot([0, 60], [model.intercept_, model.intercept_], color='gray', linewidth=2)\n",
    "plt.text(-3, 5, 'b$_{0}$')\n",
    "\n",
    "for i in range(len(x)):\n",
    "    plt.plot([x[i], x[i]], [y_pred[i], y[i]], '--', linewidth=2, color=\"lightgray\")\n",
    "\n",
    "plt.plot([], [], '--', linewidth=2, color=\"lightgray\", label='Residuals, y$_{i}$ - f(x$_{i}$)')\n",
    "\n",
    "# place the legen boc in bottom right of the graph\n",
    "plt.legend(loc='upper left', prop={'size': 9})\n",
    "\n",
    "#resize the X and Y axes\n",
    "plt.gca().xaxis.set_major_locator(plt.MultipleLocator(10))\n",
    "plt.gca().yaxis.set_major_locator(plt.MultipleLocator(10))\n",
    "\n",
    "# set lables\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "# set x, y limit\n",
    "plt.xlim([-0.5, 60.5])\n",
    "plt.ylim([-0.5, 40.5])\n",
    "\n",
    "# create the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing simple linear regression, you typically start with a given set of input-output (ùë•-ùë¶) pairs (green circles). These pairs are your observations. For example, the leftmost observation (green circle) has the input ùë• = 5 and the actual output (response) ùë¶ = 5. The next one has ùë• = 15 and ùë¶ = 20, and so on.\n",
    "\n",
    "The estimated regression function (black line) has the equation ùëì(ùë•) = ùëè‚ÇÄ + ùëè‚ÇÅùë•. Your goal is to calculate the optimal values of the predicted weights ùëè‚ÇÄ and ùëè‚ÇÅ that minimize SSR and determine the estimated regression function. The value of ùëè‚ÇÄ, also called the intercept, shows the point where the estimated regression line crosses the ùë¶ axis. It is the value of the estimated response ùëì(ùë•) for ùë• = 0. The value of ùëè‚ÇÅ determines the slope of the estimated regression line.\n",
    "\n",
    "The predicted responses (red squares) are the points on the regression line that correspond to the input values. For example, for the input ùë• = 5, the predicted response is ùëì(5) = 8.33 (represented with the leftmost red square).\n",
    "\n",
    "The residuals (vertical dashed gray lines) can be calculated as ùë¶·µ¢ - ùëì(ùê±·µ¢) = ùë¶·µ¢ - ùëè‚ÇÄ - ùëè‚ÇÅùë•·µ¢ for ùëñ = 1, ‚Ä¶, ùëõ. They are the distances between the green circles and red squares. When you implement linear regression, you are actually trying to minimize these distances and make the red squares as close to the predefined green circles as possible.\n",
    "\n",
    "### Multiple Linear Regression\n",
    "Multiple or multivariate linear regression is a case of linear regression with two or more independent variables.\n",
    "\n",
    "If there are just two independent variables, the estimated regression function is ùëì(ùë•‚ÇÅ, ùë•‚ÇÇ) = ùëè‚ÇÄ + ùëè‚ÇÅùë•‚ÇÅ + ùëè‚ÇÇùë•‚ÇÇ. It represents a regression plane in a three-dimensional space. The goal of regression is to determine the values of the weights ùëè‚ÇÄ, ùëè‚ÇÅ, and ùëè‚ÇÇ such that this plane is as close as possible to the actual responses and yield the minimal SSR.\n",
    "\n",
    "The case of more than two independent variables is similar, but more general. The estimated regression function is ùëì(ùë•‚ÇÅ, ‚Ä¶, ùë•·µ£) = ùëè‚ÇÄ + ùëè‚ÇÅùë•‚ÇÅ + ‚ãØ +ùëè·µ£ùë•·µ£, and there are ùëü + 1 weights to be determined when the number of inputs is ùëü.\n",
    "\n",
    "### Polynomial Regression\n",
    "You can regard polynomial regression as a generalized case of linear regression. You assume the polynomial dependence between the output and inputs and, consequently, the polynomial estimated regression function.\n",
    "\n",
    "In other words, in addition to linear terms like ùëè‚ÇÅùë•‚ÇÅ, your regression function ùëì can include non-linear terms such as ùëè‚ÇÇùë•‚ÇÅ¬≤, ùëè‚ÇÉùë•‚ÇÅ¬≥, or even ùëè‚ÇÑùë•‚ÇÅùë•‚ÇÇ, ùëè‚ÇÖùë•‚ÇÅ¬≤ùë•‚ÇÇ, and so on.\n",
    "\n",
    "The simplest example of polynomial regression has a single independent variable, and the estimated regression function is a polynomial of degree 2: ùëì(ùë•) = ùëè‚ÇÄ + ùëè‚ÇÅùë• + ùëè‚ÇÇùë•¬≤.\n",
    "\n",
    "Now, remember that you want to calculate ùëè‚ÇÄ, ùëè‚ÇÅ, and ùëè‚ÇÇ, which minimize SSR. These are your unknowns!\n",
    "\n",
    "Keeping this in mind, compare the previous regression function with the function ùëì(ùë•‚ÇÅ, ùë•‚ÇÇ) = ùëè‚ÇÄ + ùëè‚ÇÅùë•‚ÇÅ + ùëè‚ÇÇùë•‚ÇÇ used for linear regression. They look very similar and are both linear functions of the unknowns ùëè‚ÇÄ, ùëè‚ÇÅ, and ùëè‚ÇÇ. This is why you can solve the polynomial regression problem as a linear problem with the term ùë•¬≤ regarded as an input variable.\n",
    "\n",
    "In the case of two variables and the polynomial of degree 2, the regression function has this form: ùëì(ùë•‚ÇÅ, ùë•‚ÇÇ) = ùëè‚ÇÄ + ùëè‚ÇÅùë•‚ÇÅ + ùëè‚ÇÇùë•‚ÇÇ + ùëè‚ÇÉùë•‚ÇÅ¬≤ + ùëè‚ÇÑùë•‚ÇÅùë•‚ÇÇ + ùëè‚ÇÖùë•‚ÇÇ¬≤. The procedure for solving the problem is identical to the previous case. You apply linear regression for five inputs: ùë•‚ÇÅ, ùë•‚ÇÇ, ùë•‚ÇÅ¬≤, ùë•‚ÇÅùë•‚ÇÇ, and ùë•‚ÇÇ¬≤. What you get as the result of regression are the values of six weights which minimize SSR: ùëè‚ÇÄ, ùëè‚ÇÅ, ùëè‚ÇÇ, ùëè‚ÇÉ, ùëè‚ÇÑ, and ùëè‚ÇÖ.\n",
    "\n",
    "Of course, there are more general problems, but this should be enough to illustrate the point.\n",
    "\n",
    "### Underfitting and Overfitting\n",
    "One very important question that might arise when you‚Äôre implementing polynomial regression is related to the choice of the optimal degree of the polynomial regression function.\n",
    "\n",
    "There is no straightforward rule for doing this. It depends on the case. You should, however, be aware of two problems that might follow the choice of the degree: underfitting and overfitting.\n",
    "\n",
    "Underfitting occurs when a model can‚Äôt accurately capture the dependencies among data, usually as a consequence of its own simplicity. It often yields a low ùëÖ¬≤ with known data and bad generalization capabilities when applied with new data.\n",
    "\n",
    "Overfitting happens when a model learns both dependencies among data and random fluctuations. In other words, a model learns the existing data too well. Complex models, which have many features or terms, are often prone to overfitting. When applied to known data, such models usually yield high ùëÖ¬≤. However, they often don‚Äôt generalize well and have significantly lower ùëÖ¬≤ when used with new data.\n",
    "\n",
    "The next figure illustrates the underfitted, well-fitted, and overfitted models:\n",
    "![poly-reg.png](poly-reg.png)\n",
    "\n",
    "The top left plot shows a linear regression line that has a low ùëÖ¬≤. It might also be important that a straight line can‚Äôt take into account the fact that the actual response increases as ùë• moves away from 25 towards zero. This is likely an example of underfitting.\n",
    "\n",
    "The top right plot illustrates polynomial regression with the degree equal to 2. In this instance, this might be the optimal degree for modeling this data. The model has a value of ùëÖ¬≤ that is satisfactory in many cases and shows trends nicely.\n",
    "\n",
    "The bottom left plot presents polynomial regression with the degree equal to 3. The value of ùëÖ¬≤ is higher than in the preceding cases. This model behaves better with known data than the previous ones. However, it shows some signs of overfitting, especially for the input values close to 60 where the line starts decreasing, although actual data don‚Äôt show that.\n",
    "\n",
    "Finally, on the bottom right plot, you can see the perfect fit: six points and the polynomial line of the degree 5 (or higher) yield ùëÖ¬≤ = 1. Each actual response equals its corresponding prediction.\n",
    "\n",
    "In some situations, this might be exactly what you‚Äôre looking for. In many cases, however, this is an overfitted model. It is likely to have poor behavior with unseen data, especially with the inputs larger than 50.\n",
    "\n",
    "For example, it assumes, without any evidence, that there is a significant drop in responses for ùë• > 50 and that ùë¶ reaches zero for ùë• near 60. Such behavior is the consequence of excessive effort to learn and fit the existing data.\n",
    "\n",
    "There are a lot of resources where you can find more information about regression in general and linear regression in particular. The [regression analysis page on Wikipedia](https://en.wikipedia.org/wiki/Regression_analysis), [Wikipedia‚Äôs linear regression article](https://en.wikipedia.org/wiki/Linear_regression), as well as Khan [Academy‚Äôs linear regression article](https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/introduction-to-trend-lines/a/linear-regression-review) are good starting points.\n",
    "\n",
    "## Implementing Linear Regression in Python\n",
    "It‚Äôs time to start implementing linear regression in Python. Basically, all you should do is apply the proper packages and their functions and classes.\n",
    "\n",
    "### Python Packages for Linear Regression\n",
    "The package NumPy is a fundamental Python scientific package that allows many high-performance operations on single- and multi-dimensional arrays. It also offers many mathematical routines. Of course, it‚Äôs open source.\n",
    "\n",
    "If you‚Äôre not familiar with NumPy, you can use the official [NumPy User Guide](https://docs.scipy.org/doc/numpy/user/index.html) and read [Look Ma, No For-Loops: Array Programming With NumPy](https://realpython.com/numpy-array-programming/). In addition, [Pure Python vs NumPy vs TensorFlow Performance Comparison](https://realpython.com/numpy-tensorflow-performance/) can give you a pretty good idea on the performance gains you can achieve when applying NumPy.\n",
    "\n",
    "The package scikit-learn is a widely used Python library for machine learning, built on top of NumPy and some other packages. It provides the means for preprocessing data, reducing dimensionality, implementing regression, classification, clustering, and more. Like NumPy, scikit-learn is also open source.\n",
    "\n",
    "You can check the page [Generalized Linear Models](https://scikit-learn.org/stable/modules/linear_model.html) on the [scikit-learn web site](https://scikit-learn.org/stable/) to learn more about linear models and get deeper insight into how this package works.\n",
    "\n",
    "If you want to implement linear regression and need the functionality beyond the scope of scikit-learn, you should consider statsmodels. It‚Äôs a powerful Python package for the estimation of statistical models, performing tests, and more. It‚Äôs open source as well.\n",
    "\n",
    "You can find more information on statsmodels on its [official web site](https://www.statsmodels.org/stable/index.html).\n",
    "\n",
    "### Simple Linear Regression With scikit-learn\n",
    "Let‚Äôs start with the simplest case, which is simple linear regression.\n",
    "\n",
    "There are five basic steps when you‚Äôre implementing linear regression:\n",
    "- Import the packages and classes you need.\n",
    "- Provide data to work with and eventually do appropriate transformations.\n",
    "- Create a regression model and fit it with existing data.\n",
    "- Check the results of model fitting to know whether the model is satisfactory.\n",
    "- Apply the model for predictions.\n",
    "\n",
    "These steps are more or less general for most of the regression approaches and implementations.\n",
    "\n",
    "#### Step 1: Import packages and classes\n",
    "\n",
    "The first step is to import the package numpy and the class LinearRegression from sklearn.linear_model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you have all the functionalities you need to implement linear regression.\n",
    "\n",
    "The fundamental data type of NumPy is the array type called numpy.ndarray. The rest of this article uses the term array to refer to instances of the type numpy.ndarray.\n",
    "\n",
    "The class sklearn.linear_model.LinearRegression will be used to perform linear and polynomial regression and make predictions accordingly.\n",
    "\n",
    "#### Step 2: Provide data\n",
    "\n",
    "The second step is defining data to work with. The inputs (predictors, ùë•) and output (response, ùë¶) should be arrays (the instances of the class numpy.ndarray) or similar objects. This is the simplest way of providing data for regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))\n",
    "y = np.array([5, 20, 14, 32, 22, 38])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you have two arrays: the input x and output y. You should call .reshape() on x because this array is required to be two-dimensional, or to be more precise, to have one column and as many rows as necessary. That‚Äôs exactly what the argument (-1, 1) of .reshape() specifies.\n",
    "\n",
    "This is how x and y look now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5],\n",
       "       [15],\n",
       "       [25],\n",
       "       [35],\n",
       "       [45],\n",
       "       [55]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 20, 14, 32, 22, 38])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, x has two dimensions, and x.shape is (6, 1), while y has a single dimension, and y.shape is (6,).\n",
    "\n",
    "#### Step 3: Create a model and fit it\n",
    "\n",
    "The next step is to create a linear regression model and fit it using the existing data.\n",
    "\n",
    "Let‚Äôs create an instance of the class LinearRegression, which will represent the regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This statement creates the variable model as the instance of LinearRegression. You can provide several optional parameters to LinearRegression:\n",
    "- fit_intercept is a Boolean (True by default) that decides whether to calculate the intercept ùëè‚ÇÄ (True) or consider it equal to zero (False).\n",
    "- normalize is a Boolean (False by default) that decides whether to normalize the input variables (True) or not (False).\n",
    "- copy_X is a Boolean (True by default) that decides whether to copy (True) or overwrite the input variables (False).\n",
    "- n_jobs is an integer or None (default) and represents the number of jobs used in parallel computation. None usually means one job and -1 to use all processors.\n",
    "\n",
    "This example uses the default values of all parameters.\n",
    "\n",
    "It‚Äôs time to start using the model. First, you need to call .fit() on model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With .fit(), you calculate the optimal values of the weights ùëè‚ÇÄ and ùëè‚ÇÅ, using the existing input and output (x and y) as the arguments. In other words, .fit() fits the model. It returns self, which is the variable model itself. That‚Äôs why you can replace the last two statements with this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This statement does the same thing as the previous two. It‚Äôs just shorter.\n",
    "\n",
    "#### Step 4: Get results\n",
    "\n",
    "Once you have your model fitted, you can get the results to check whether the model works satisfactorily and interpret it.\n",
    "\n",
    "You can obtain the coefficient of determination (ùëÖ¬≤) with .score() called on model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient of determination: 0.715875613748\n"
     ]
    }
   ],
   "source": [
    "r_sq = model.score(x, y)\n",
    "print('coefficient of determination:', r_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you‚Äôre applying .score(), the arguments are also the predictor x and regressor y, and the return value is ùëÖ¬≤.\n",
    "\n",
    "The attributes of model are .intercept_, which represents the coefficient, ùëè‚ÇÄ and .coef_, which represents ùëè‚ÇÅ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: 5.63333333333\n",
      "slope: [ 0.54]\n"
     ]
    }
   ],
   "source": [
    "print('intercept:', model.intercept_)\n",
    "\n",
    "print('slope:', model.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above illustrates how to get ùëè‚ÇÄ and ùëè‚ÇÅ. You can notice that .intercept_ is a scalar, while .coef_ is an array.\n",
    "\n",
    "The value ùëè‚ÇÄ = 5.63 (approximately) illustrates that your model predicts the response 5.63 when ùë• is zero. The value ùëè‚ÇÅ = 0.54 means that the predicted response rises by 0.54 when ùë• is increased by one.\n",
    "\n",
    "You should notice that you can provide y as a two-dimensional array as well. In this case, you‚Äôll get a similar result. This is how it might look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: [ 5.63333333]\n",
      "slope: [[ 0.54]]\n"
     ]
    }
   ],
   "source": [
    "new_model = LinearRegression().fit(x, y.reshape((-1, 1)))\n",
    "print('intercept:', new_model.intercept_)\n",
    "\n",
    "print('slope:', new_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this example is very similar to the previous one, but in this case, .intercept_ is a one-dimensional array with the single element ùëè‚ÇÄ, and .coef_ is a two-dimensional array with the single element ùëè‚ÇÅ.\n",
    "\n",
    "#### Step 5: Predict response\n",
    "\n",
    "Once there is a satisfactory model, you can use it for predictions with either existing or new data.\n",
    "\n",
    "To obtain the predicted response, use .predict():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted response:\n",
      "[  8.33333333  13.73333333  19.13333333  24.53333333  29.93333333\n",
      "  35.33333333]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x)\n",
    "print('predicted response:', y_pred, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applying .predict(), you pass the regressor as the argument and get the corresponding predicted response.\n",
    "\n",
    "This is a nearly identical way to predict the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted response:\n",
      "[[  8.33333333]\n",
      " [ 13.73333333]\n",
      " [ 19.13333333]\n",
      " [ 24.53333333]\n",
      " [ 29.93333333]\n",
      " [ 35.33333333]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.intercept_ + model.coef_ * x\n",
    "print('predicted response:', y_pred, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, you multiply each element of x with model.coef_ and add model.intercept_ to the product.\n",
    "\n",
    "The output here differs from the previous example only in dimensions. The predicted response is now a two-dimensional array, while in the previous case, it had one dimension.\n",
    "\n",
    "If you reduce the number of dimensions of x to one, these two approaches will yield the same result. You can do this by replacing x with x.reshape(-1), x.flatten(), or x.ravel() when multiplying it with model.coef_.\n",
    "\n",
    "In practice, regression models are often applied for forecasts. This means that you can use fitted models to calculate the outputs based on some other, new inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "[ 5.63333333  6.17333333  6.71333333  7.25333333  7.79333333]\n"
     ]
    }
   ],
   "source": [
    "x_new = np.arange(5).reshape((-1, 1))\n",
    "print(x_new)\n",
    "\n",
    "y_new = model.predict(x_new)\n",
    "print(y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here .predict() is applied to the new regressor x_new and yields the response y_new. This example conveniently uses [arange()](https://realpython.com/how-to-use-numpy-arange/) from numpy to generate an array with the elements from 0 (inclusive) to 5 (exclusive), that is 0, 1, 2, 3, and 4.\n",
    "\n",
    "You can find more information about LinearRegression on the [official documentation page](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression With scikit-learn\n",
    "You can implement multiple linear regression following the same steps as you would for simple regression.\n",
    "\n",
    "#### Steps 1 and 2: Import packages and classes, and provide data\n",
    "\n",
    "First, you import numpy and sklearn.linear_model.LinearRegression and provide known inputs and output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x = [[0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]]\n",
    "y = [4, 5, 20, 14, 32, 22, 38, 43]\n",
    "x, y = np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That‚Äôs a simple way to define the input x and output y. You can print x and y to see how they look now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1],\n",
       "       [ 5,  1],\n",
       "       [15,  2],\n",
       "       [25,  5],\n",
       "       [35, 11],\n",
       "       [45, 15],\n",
       "       [55, 34],\n",
       "       [60, 35]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  5, 20, 14, 32, 22, 38, 43])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multiple linear regression, x is a two-dimensional array with at least two columns, while y is usually a one-dimensional array. This is a simple example of multiple linear regression, and x has exactly two columns.\n",
    "\n",
    "#### Step 3: Create a model and fit it\n",
    "\n",
    "The next step is to create the regression model as an instance of LinearRegression and fit it with .fit():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this statement is the variable model referring to the object of type LinearRegression. It represents the regression model fitted with existing data.\n",
    "\n",
    "#### Step 4: Get results\n",
    "\n",
    "You can obtain the properties of the model the same way as in the case of simple linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient of determination: 0.861593925876\n",
      "intercept: 5.5225792752\n",
      "slope: [ 0.44706965  0.25502548]\n"
     ]
    }
   ],
   "source": [
    "r_sq = model.score(x, y)\n",
    "print('coefficient of determination:', r_sq)\n",
    "\n",
    "print('intercept:', model.intercept_)\n",
    "\n",
    "print('slope:', model.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You obtain the value of ùëÖ¬≤ using .score() and the values of the estimators of regression coefficients with .intercept_ and .coef_. Again, .intercept_ holds the bias ùëè‚ÇÄ, while now .coef_ is an array containing ùëè‚ÇÅ and ùëè‚ÇÇ respectively.\n",
    "\n",
    "In this example, the intercept is approximately 5.52, and this is the value of the predicted response when ùë•‚ÇÅ = ùë•‚ÇÇ = 0. The increase of ùë•‚ÇÅ by 1 yields the rise of the predicted response by 0.45. Similarly, when ùë•‚ÇÇ grows by 1, the response rises by 0.26.\n",
    "\n",
    "#### Step 5: Predict response\n",
    "\n",
    "Predictions also work the same way as in the case of simple linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted response:\n",
      "[  5.77760476   8.012953    12.73867497  17.9744479   23.97529728\n",
      "  29.4660957   38.78227633  41.27265006]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x)\n",
    "print('predicted response:', y_pred, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted response is obtained with .predict(), which is very similar to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted response:\n",
      "[  5.77760476   8.012953    12.73867497  17.9744479   23.97529728\n",
      "  29.4660957   38.78227633  41.27265006]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.intercept_ + np.sum(model.coef_ * x, axis=1)\n",
    "print('predicted response:', y_pred, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can predict the output values by multiplying each column of the input with the appropriate weight, summing the results and adding the intercept to the sum.\n",
    "\n",
    "You can apply this model to new data as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]\n",
      " [8 9]]\n",
      "[  5.77760476   7.18179502   8.58598528   9.99017554  11.3943658 ]\n"
     ]
    }
   ],
   "source": [
    "x_new = np.arange(10).reshape((-1, 2))\n",
    "print(x_new)\n",
    "\n",
    "y_new = model.predict(x_new)\n",
    "print(y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That‚Äôs the prediction using a linear regression model.\n",
    "\n",
    "### Polynomial Regression With scikit-learn\n",
    "Implementing polynomial regression with scikit-learn is very similar to linear regression. There is only one extra step: you need to transform the array of inputs to include non-linear terms such as ùë•¬≤.\n",
    "\n",
    "#### Step 1: Import packages and classes\n",
    "\n",
    "In addition to numpy and sklearn.linear_model.LinearRegression, you should also import the class PolynomialFeatures from sklearn.preprocessing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The import is now done, and you have everything you need to work with.\n",
    "\n",
    "#### Step 2a: Provide data\n",
    "\n",
    "This step defines the input and output and is the same as in the case of linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))\n",
    "y = np.array([15, 11, 2, 8, 25, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have the input and output in a suitable format. Keep in mind that you need the input to be a two-dimensional array. That‚Äôs why .reshape() is used.\n",
    "\n",
    "#### Step 2b: Transform input data\n",
    "\n",
    "This is the new step you need to implement for polynomial regression!\n",
    "\n",
    "As you‚Äôve seen earlier, you need to include ùë•¬≤ (and perhaps other terms) as additional features when implementing polynomial regression. For that reason, you should transform the input array x to contain the additional column(s) with the values of ùë•¬≤ (and eventually more features).\n",
    "\n",
    "It‚Äôs possible to transform the input array in several ways (like using insert() from numpy), but the class PolynomialFeatures is very convenient for this purpose. Let‚Äôs create an instance of this class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = PolynomialFeatures(degree=2, include_bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable transformer refers to an instance of PolynomialFeatures which you can use to transform the input x.\n",
    "\n",
    "You can provide several optional parameters to PolynomialFeatures:\n",
    "- degree is an integer (2 by default) that represents the degree of the polynomial regression function.\n",
    "- interaction_only is a Boolean (False by default) that decides whether to include only interaction features (True) or all features (False).\n",
    "- include_bias is a Boolean (True by default) that decides whether to include the bias (intercept) column of ones (True) or not (False).\n",
    "\n",
    "This example uses the default values of all parameters, but you‚Äôll sometimes want to experiment with the degree of the function, and it can be beneficial to provide this argument anyway.\n",
    "\n",
    "Before applying transformer, you need to fit it with .fit():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.fit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once transformer is fitted, it‚Äôs ready to create a new, modified input. You apply .transform() to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = transformer.transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That‚Äôs the transformation of the input array with .transform(). It takes the input array as the argument and returns the modified array.\n",
    "\n",
    "You can also use .fit_transform() to replace the three previous statements with only one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That‚Äôs fitting and transforming the input array in one statement with .fit_transform(). It also takes the input array and effectively does the same thing as .fit() and .transform() called in that order. It also returns the modified array. This is how the new input array looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    5.,    25.],\n",
       "       [   15.,   225.],\n",
       "       [   25.,   625.],\n",
       "       [   35.,  1225.],\n",
       "       [   45.,  2025.],\n",
       "       [   55.,  3025.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modified input array contains two columns: one with the original inputs and the other with their squares.\n",
    "\n",
    "You can find more information about PolynomialFeatures on the [official documentation page](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html).\n",
    "\n",
    "#### Step 3: Create a model and fit it\n",
    "\n",
    "This step is also the same as in the case of linear regression. You create and fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(x_, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression model is now created and fitted. It‚Äôs ready for application.\n",
    "\n",
    "You should keep in mind that the first argument of .fit() is the modified input array x_ and not the original x.\n",
    "\n",
    "#### Step 4: Get results\n",
    "\n",
    "You can obtain the properties of the model the same way as in the case of linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient of determination: 0.89085162625\n",
      "intercept: 21.3723214286\n",
      "coefficients: [-1.32357143  0.02839286]\n"
     ]
    }
   ],
   "source": [
    "r_sq = model.score(x_, y)\n",
    "print('coefficient of determination:', r_sq)\n",
    "\n",
    "print('intercept:', model.intercept_)\n",
    "\n",
    "print('coefficients:', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, .score() returns ùëÖ¬≤. Its first argument is also the modified input x_, not x. The values of the weights are associated to .intercept_ and .coef_: .intercept_ represents ùëè‚ÇÄ, while .coef_ references the array that contains ùëè‚ÇÅ and ùëè‚ÇÇ respectively.\n",
    "\n",
    "You can obtain a very similar result with different transformation and regression arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = PolynomialFeatures(degree=2, include_bias=True).fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you call PolynomialFeatures with the default parameter include_bias=True (or if you just omit it), you‚Äôll obtain the new input array x_ with the additional leftmost column containing only ones. This column corresponds to the intercept. This is how the modified input array looks in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   5.00000000e+00,   2.50000000e+01],\n",
       "       [  1.00000000e+00,   1.50000000e+01,   2.25000000e+02],\n",
       "       [  1.00000000e+00,   2.50000000e+01,   6.25000000e+02],\n",
       "       [  1.00000000e+00,   3.50000000e+01,   1.22500000e+03],\n",
       "       [  1.00000000e+00,   4.50000000e+01,   2.02500000e+03],\n",
       "       [  1.00000000e+00,   5.50000000e+01,   3.02500000e+03]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column of x_ contains ones, the second has the values of x, while the third holds the squares of x.\n",
    "\n",
    "The intercept is already included with the leftmost column of ones, and you don‚Äôt need to include it again when creating the instance of LinearRegression. Thus, you can provide fit_intercept=False. This is how the next statement looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=False).fit(x_, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable model again corresponds to the new input array x_. Therefore x_ should be passed as the first argument instead of x.\n",
    "\n",
    "This approach yields the following results, which are similar to the previous case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient of determination: 0.89085162625\n",
      "intercept: 0.0\n",
      "coefficients: [ 21.37232143  -1.32357143   0.02839286]\n"
     ]
    }
   ],
   "source": [
    "r_sq = model.score(x_, y)\n",
    "print('coefficient of determination:', r_sq)\n",
    "\n",
    "print('intercept:', model.intercept_)\n",
    "\n",
    "print('coefficients:', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that now .intercept_ is zero, but .coef_ actually contains ùëè‚ÇÄ as its first element. Everything else is the same.\n",
    "\n",
    "#### Step 5: Predict response\n",
    "\n",
    "If you want to get the predicted response, just use .predict(), but remember that the argument should be the modified input x_ instead of the old x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted response:\n",
      "[ 15.46428571   7.90714286   6.02857143   9.82857143  19.30714286\n",
      "  34.46428571]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_)\n",
    "print('predicted response:', y_pred, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the prediction works almost the same way as in the case of linear regression. It just requires the modified input instead of the original.\n",
    "\n",
    "You can apply the identical procedure if you have several input variables. You‚Äôll have an input array with more than one column, but everything else is the same. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import packages\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Step 2a: Provide data\n",
    "x = [[0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]]\n",
    "y = [4, 5, 20, 14, 32, 22, 38, 43]\n",
    "x, y = np.array(x), np.array(y)\n",
    "\n",
    "# Step 2b: Transform input data\n",
    "x_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(x)\n",
    "\n",
    "# Step 3: Create a model and fit it\n",
    "model = LinearRegression().fit(x_, y)\n",
    "\n",
    "# Step 4: Get results\n",
    "r_sq = model.score(x_, y)\n",
    "intercept, coefficients = model.intercept_, model.coef_\n",
    "\n",
    "# Step 5: Predict\n",
    "y_pred = model.predict(x_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This regression example yields the following results and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient of determination: 0.945370144913\n",
      "intercept: 0.84305564524\n",
      "coefficients:\n",
      "[ 2.44828275  0.16160353 -0.15259677  0.47928683 -0.4641851 ]\n",
      "predicted response:\n",
      "[  0.54047408  11.36340283  16.07809622  15.79139     29.73858619\n",
      "  23.50834636  39.05631386  41.92339046]\n"
     ]
    }
   ],
   "source": [
    "print('coefficient of determination:', r_sq)\n",
    "\n",
    "print('intercept:', intercept)\n",
    "\n",
    "print('coefficients:', coefficients, sep='\\n')\n",
    "\n",
    "\n",
    "print('predicted response:', y_pred, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, there are six regression coefficients (including the intercept), as shown in the estimated regression function ùëì(ùë•‚ÇÅ, ùë•‚ÇÇ) = ùëè‚ÇÄ + ùëè‚ÇÅùë•‚ÇÅ + ùëè‚ÇÇùë•‚ÇÇ + ùëè‚ÇÉùë•‚ÇÅ¬≤ + ùëè‚ÇÑùë•‚ÇÅùë•‚ÇÇ + ùëè‚ÇÖùë•‚ÇÇ¬≤.\n",
    "\n",
    "You can also notice that polynomial regression yielded a higher coefficient of determination than multiple linear regression for the same problem. At first, you could think that obtaining such a large ùëÖ¬≤ is an excellent result. It might be.\n",
    "\n",
    "However, in real-world situations, having a complex model and ùëÖ¬≤ very close to 1 might also be a sign of overfitting. To check the performance of a model, you should test it with new data, that is with observations not used to fit (train) the model. To learn how to split your dataset into the training and test subsets, check out [Split Your Dataset With scikit-learn‚Äôs train_test_split()](https://realpython.com/train-test-split-python-data/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Linear Regression With statsmodels\n",
    "You can implement linear regression in Python relatively easily by using the package statsmodels as well. Typically, this is desirable when there is a need for more detailed results.\n",
    "\n",
    "The procedure is similar to that of scikit-learn.\n",
    "\n",
    "#### Step 1: Import packages\n",
    "\n",
    "First you need to do some imports. In addition to numpy, you need to import statsmodels.api:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have the packages you need.\n",
    "\n",
    "#### Step 2: Provide data and transform inputs\n",
    "\n",
    "You can provide the inputs and outputs the same way as you did when you were using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]]\n",
    "y = [4, 5, 20, 14, 32, 22, 38, 43]\n",
    "x, y = np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input and output arrays are created, but the job is not done yet.\n",
    "\n",
    "You need to add the column of ones to the inputs if you want statsmodels to calculate the intercept ùëè‚ÇÄ. It doesn‚Äôt takes ùëè‚ÇÄ into account by default. This is just one function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sm.add_constant(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That‚Äôs how you add the column of ones to x with add_constant(). It takes the input array x as an argument and returns a new array with the column of ones inserted at the beginning. This is how x and y look now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.,   0.,   1.],\n",
       "       [  1.,   5.,   1.],\n",
       "       [  1.,  15.,   2.],\n",
       "       [  1.,  25.,   5.],\n",
       "       [  1.,  35.,  11.],\n",
       "       [  1.,  45.,  15.],\n",
       "       [  1.,  55.,  34.],\n",
       "       [  1.,  60.,  35.]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  5, 20, 14, 32, 22, 38, 43])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the modified x has three columns: the first column of ones (corresponding to ùëè‚ÇÄ and replacing the intercept) as well as two columns of the original features.\n",
    "\n",
    "#### Step 3: Create a model and fit it\n",
    "\n",
    "The regression model based on ordinary least squares is an instance of the class statsmodels.regression.linear_model.OLS. This is how you can obtain one:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be careful here! Please, notice that the first argument is the output, followed with the input. There are several more optional parameters.\n",
    "\n",
    "To find more information about this class, please visit the [official documentation page](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.html).\n",
    "\n",
    "Once your model is created, you can apply .fit() on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling .fit(), you obtain the variable results, which is an instance of the class statsmodels.regression.linear_model.RegressionResultsWrapper. This object holds a lot of information about the regression model.\n",
    "\n",
    "#### Step 4: Get results\n",
    "\n",
    "The variable results refers to the object that contains detailed information about the results of linear regression. Explaining them is far beyond the scope of this article, but you‚Äôll learn here how to extract them.\n",
    "\n",
    "You can call .summary() to get the table with the results of linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.862\n",
      "Model:                            OLS   Adj. R-squared:                  0.806\n",
      "Method:                 Least Squares   F-statistic:                     15.56\n",
      "Date:                Mon, 11 Jan 2021   Prob (F-statistic):            0.00713\n",
      "Time:                        03:22:30   Log-Likelihood:                -24.316\n",
      "No. Observations:                   8   AIC:                             54.63\n",
      "Df Residuals:                       5   BIC:                             54.87\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          5.5226      4.431      1.246      0.268      -5.867      16.912\n",
      "x1             0.4471      0.285      1.567      0.178      -0.286       1.180\n",
      "x2             0.2550      0.453      0.563      0.598      -0.910       1.420\n",
      "==============================================================================\n",
      "Omnibus:                        0.561   Durbin-Watson:                   3.268\n",
      "Prob(Omnibus):                  0.755   Jarque-Bera (JB):                0.534\n",
      "Skew:                           0.380   Prob(JB):                        0.766\n",
      "Kurtosis:                       1.987   Cond. No.                         80.1\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/scipy/stats/stats.py:1394: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=8\n",
      "  \"anyway, n=%i\" % int(n))\n"
     ]
    }
   ],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table is very comprehensive. You can find many statistical values associated with linear regression including ùëÖ¬≤, ùëè‚ÇÄ, ùëè‚ÇÅ, and ùëè‚ÇÇ.\n",
    "\n",
    "In this particular case, you might obtain the warning related to kurtosistest. This is due to the small number of observations provided.\n",
    "\n",
    "You can extract any of the values from the table above. Here‚Äôs an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient of determination: 0.861593925876\n",
      "adjusted coefficient of determination: 0.806231496226\n",
      "regression coefficients: [ 5.52257928  0.44706965  0.25502548]\n"
     ]
    }
   ],
   "source": [
    "print('coefficient of determination:', results.rsquared)\n",
    "\n",
    "print('adjusted coefficient of determination:', results.rsquared_adj)\n",
    "\n",
    "print('regression coefficients:', results.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That‚Äôs how you obtain some of the results of linear regression:\n",
    "- .rsquared holds ùëÖ¬≤.\n",
    "- rsquared_adj represents adjusted ùëÖ¬≤ (ùëÖ¬≤ corrected according to the number of input features).\n",
    "- .params refers the array with ùëè‚ÇÄ, ùëè‚ÇÅ, and ùëè‚ÇÇ respectively.\n",
    "\n",
    "You can also notice that these results are identical to those obtained with scikit-learn for the same problem.\n",
    "\n",
    "To find more information about the results of linear regression, please visit [the official documentation page](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.RegressionResults.html).\n",
    "\n",
    "#### Step 5: Predict response\n",
    "\n",
    "You can obtain the predicted response on the input values used for creating the model using .fittedvalues or .predict() with the input array as the argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted response:\n",
      "[  5.77760476   8.012953    12.73867497  17.9744479   23.97529728\n",
      "  29.4660957   38.78227633  41.27265006]\n",
      "predicted response:\n",
      "[  5.77760476   8.012953    12.73867497  17.9744479   23.97529728\n",
      "  29.4660957   38.78227633  41.27265006]\n"
     ]
    }
   ],
   "source": [
    "print('predicted response:', results.fittedvalues, sep='\\n')\n",
    "\n",
    "print('predicted response:', results.predict(x), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the predicted response for known inputs. If you want predictions with new regressors, you can also apply .predict() with new data as the argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  1.]\n",
      " [ 1.  2.  3.]\n",
      " [ 1.  4.  5.]\n",
      " [ 1.  6.  7.]\n",
      " [ 1.  8.  9.]]\n",
      "[  5.77760476   7.18179502   8.58598528   9.99017554  11.3943658 ]\n"
     ]
    }
   ],
   "source": [
    "x_new = sm.add_constant(np.arange(10).reshape((-1, 2)))\n",
    "print(x_new)\n",
    "\n",
    "y_new = results.predict(x_new)\n",
    "print(y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can notice that the predicted results are the same as those obtained with scikit-learn for the same problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Linear Regression\n",
    "Linear regression is sometimes not appropriate, especially for non-linear models of high complexity.\n",
    "\n",
    "Fortunately, there are other regression techniques suitable for the cases where linear regression doesn‚Äôt work well. Some of them are support vector machines, decision trees, random forest, and neural networks.\n",
    "\n",
    "There are numerous Python libraries for regression using these techniques. Most of them are free and open-source. That‚Äôs one of the reasons why Python is among the main programming languages for machine learning.\n",
    "\n",
    "The package scikit-learn provides the means for using other regression techniques in a very similar way to what you‚Äôve seen. It contains the classes for support vector machines, decision trees, random forest, and more, with the methods .fit(), .predict(), .score() and so on.\n",
    "\n",
    "## Conclusion\n",
    "You now know what linear regression is and how you can implement it with Python and three open-source packages: NumPy, scikit-learn, and statsmodels.\n",
    "\n",
    "You use NumPy for handling arrays.\n",
    "\n",
    "Linear regression is implemented with the following:\n",
    "- scikit-learn if you don‚Äôt need detailed results and want to use the approach consistent with other regression techniques\n",
    "- statsmodels if you need the advanced statistical parameters of a model\n",
    "\n",
    "Both approaches are worth learning how to use and exploring further. The links in this article can be very useful for that.\n",
    "\n",
    "When performing linear regression in Python, you can follow these steps:\n",
    "- Import the packages and classes you need\n",
    "- Provide data to work with and eventually do appropriate transformations\n",
    "- Create a regression model and fit it with existing data\n",
    "- Check the results of model fitting to know whether the model is satisfactory\n",
    "- Apply the model for predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "[source](https://realpython.com/linear-regression-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scatter plot matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scatter plot matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np    \n",
    "\n",
    "a = np.matrix('''\n",
    "\t0 0 0 0 1 1 1 1 1 1; \n",
    "\t0 0 0 0 1 1 1 1 1 1; \n",
    "\t0 0 0 0 0 1 1 1 1 1; \n",
    "\t0 0 0 0 0 0 1 1 1 1; \n",
    "\t0 0 0 0 0 0 1 1 1 1; \n",
    "\t0 0 0 0 0 0 0 1 1 1; \n",
    "\t0 0 0 0 0 0 0 0 1 1; \n",
    "\t0 0 0 0 0 0 0 0 1 1; \n",
    "\t0 0 0 0 0 0 0 0 0 1; \n",
    "\t0 0 0 0 0 0 0 0 0 0 \n",
    "\t''')\n",
    "a = np.flip(a, 0)\n",
    "\n",
    "def scatterplot_matrix(data):\n",
    "    x2, x1 = data.shape\n",
    "    # Plot the data.\n",
    "    colors = np.array(['b', 'g'])\n",
    "    fillstyles = np.array(['none', 'full'])\n",
    "    for i in range(0, x1):\n",
    "        for j in range(0, x2):\n",
    "            plt.plot(i,j, marker='o', fillstyle=fillstyles[data[j,i]], markersize=10, color=colors[data[j,i]][0][0])\n",
    "\n",
    "            \n",
    "# set the style and title\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.title('Scatter Plot Matrix')\n",
    "\n",
    "# prepare the plot\n",
    "scatterplot_matrix(a)\n",
    "\n",
    "#resize the X and Y axes\n",
    "plt.gca().xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "plt.gca().yaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "\n",
    "# set lables\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "\n",
    "# set x, y limit\n",
    "plt.xlim([-0.5, 9.9])\n",
    "plt.ylim([-0.5, 9.9])\n",
    "\n",
    "# create the graph\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
